{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aydin-ab/CV_DMTet/blob/main/DMTet_Tut_(David).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Af_jBwcMwG8"
      },
      "source": [
        "# Installing Dependencies: Kaolin and PyTorch3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9x7VxtwyMthl",
        "outputId": "c4e0212f-0119-46e6-de4c-9757ac432653"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/CV_DMTet\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive, auth\n",
        "\n",
        "# from googleapiclient.discovery import build\n",
        "# auth.authenticate_user()\n",
        "# drive_service = build('drive', 'v3')\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "from pathlib import Path\n",
        "INSTALL_PATH = Path(\"/content/drive/MyDrive/CV_DMTet/\")\n",
        "%cd  $INSTALL_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fomq21yA4wP8",
        "outputId": "c26384c1-64b5-4a0f-dead-2db847c1908d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: Cython 0.29.32\n",
            "Uninstalling Cython-0.29.32:\n",
            "  Successfully uninstalled Cython-0.29.32\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 14.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 24.7 MB 1.4 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# reinstall cython, install usd-core (for 3D rendering), and clone into kaolin repo\n",
        "!pip uninstall Cython --yes\n",
        "!pip install Cython==0.29.20  --quiet\n",
        "!pip install usd-core --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2gnrsxeoc_W",
        "outputId": "b4ad16a2-c022-4e90-82d6-f6ca4d4d977c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: IGNORE_TORCH_VER=1\n",
            "env: KAOLIN_INSTALL_EXPERIMENTAL=1\n",
            "/content/drive/MyDrive/CV_DMTet\n",
            "/content/drive/MyDrive/CV_DMTet/kaolin\n",
            "Checking if /content/drive/MyDrive/CV_DMTet/kaolin/kaolin/version.py exists\n"
          ]
        }
      ],
      "source": [
        "# installing kaolin and check version\n",
        "%env IGNORE_TORCH_VER=1\n",
        "%env KAOLIN_INSTALL_EXPERIMENTAL=1\n",
        "KAOLIN_PATH = INSTALL_PATH / \"kaolin\"\n",
        "%cd $INSTALL_PATH\n",
        "!if [ ! -d $KAOLIN_PATH ]; then git clone --recursive https://github.com/NVIDIAGameWorks/kaolin; fi;\n",
        "%cd $KAOLIN_PATH\n",
        "SETUP_CHECK = KAOLIN_PATH / \"kaolin\" / \"version.py\"\n",
        "!echo Checking if $SETUP_CHECK exists\n",
        "!if [ ! -f $SETUP_CHECK ]; then python setup.py develop; fi;\n",
        "# !python -c \"import kaolin; print(kaolin.__version__)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LWMtC40fLr1",
        "outputId": "25be82cd-a3ab-4da6-df81-20bf42f66dca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "setup.py:36: UserWarning: Kaolin is compatible with PyTorch >=1.6.0, <=1.13.0, but found version 1.13.0+cu116. Continuing with the installed version as IGNORE_TORCH_VER is set.\n",
            "  warnings.warn(\n",
            "Warning: passing language='c++' to cythonize() is deprecated. Instead, put \"# distutils: language=c++\" in your .pyx or .pxd file(s)\n",
            "Compiling kaolin/cython/ops/mesh/triangle_hash.pyx because it depends on /usr/local/lib/python3.8/dist-packages/Cython/Includes/libcpp/vector.pxd.\n",
            "Compiling kaolin/cython/ops/conversions/mise.pyx because it depends on /usr/local/lib/python3.8/dist-packages/Cython/Includes/libcpp/vector.pxd.\n",
            "[1/2] Cythonizing kaolin/cython/ops/conversions/mise.pyx\n",
            "/usr/local/lib/python3.8/dist-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /content/drive/MyDrive/CV_DMTet/kaolin/kaolin/cython/ops/conversions/mise.pyx\n",
            "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "warning: kaolin/cython/ops/conversions/mise.pyx:90:33: Not all members given for struct 'Voxel'\n",
            "warning: kaolin/cython/ops/conversions/mise.pyx:90:33: Not all members given for struct 'Voxel'\n",
            "warning: kaolin/cython/ops/conversions/mise.pyx:284:33: Not all members given for struct 'Voxel'\n",
            "warning: kaolin/cython/ops/conversions/mise.pyx:284:33: Not all members given for struct 'Voxel'\n",
            "[2/2] Cythonizing kaolin/cython/ops/mesh/triangle_hash.pyx\n",
            "/usr/local/lib/python3.8/dist-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /content/drive/MyDrive/CV_DMTet/kaolin/kaolin/cython/ops/mesh/triangle_hash.pyx\n",
            "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "running install_lib\n",
            "running build_py\n",
            "copying kaolin/version.py -> build/lib.linux-x86_64-3.8/kaolin\n",
            "running egg_info\n",
            "writing kaolin.egg-info/PKG-INFO\n",
            "writing dependency_links to kaolin.egg-info/dependency_links.txt\n",
            "writing requirements to kaolin.egg-info/requires.txt\n",
            "writing top-level names to kaolin.egg-info/top_level.txt\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/cpp_extension.py:476: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "reading manifest template 'MANIFEST.in'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'kaolin.egg-info/SOURCES.txt'\n",
            "copying kaolin/cython/ops/conversions/mise.cpp -> build/lib.linux-x86_64-3.8/kaolin/cython/ops/conversions\n",
            "copying kaolin/cython/ops/mesh/triangle_hash.cpp -> build/lib.linux-x86_64-3.8/kaolin/cython/ops/mesh\n",
            "running build_ext\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/cpp_extension.py:387: UserWarning: The detected CUDA version (11.2) has a minor version mismatch with the version that was used to compile PyTorch (11.6). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "building 'kaolin.ops.mesh.triangle_hash' extension\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/numpy/core/include -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/include/python3.8 -c kaolin/cython/ops/mesh/triangle_hash.cpp -o build/temp.linux-x86_64-3.8/kaolin/cython/ops/mesh/triangle_hash.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=triangle_hash -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1969:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kkaolin/cython/ops/mesh/triangle_hash.cpp:656\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\n",
            " #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\n",
            "  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kkaolin/cython/ops/mesh/triangle_hash.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KPyObject* __pyx_f_6kaolin_3ops_4mesh_13triangle_hash_12TriangleHash_query(__pyx_obj_6kaolin_3ops_4mesh_13triangle_hash_TriangleHash*, __Pyx_memviewslice, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kkaolin/cython/ops/mesh/triangle_hash.cpp:2880:33:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "   for (__pyx_t_6 = 0; \u001b[01;35m\u001b[K__pyx_t_6 < __pyx_t_19\u001b[m\u001b[K; __pyx_t_6+=1) {\n",
            "                       \u001b[01;35m\u001b[K~~~~~~~~~~^~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kkaolin/cython/ops/mesh/triangle_hash.cpp:2889:33:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "   for (__pyx_t_6 = 0; \u001b[01;35m\u001b[K__pyx_t_6 < __pyx_t_19\u001b[m\u001b[K; __pyx_t_6+=1) {\n",
            "                       \u001b[01;35m\u001b[K~~~~~~~~~~^~~~~~~~~~~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/kaolin/cython/ops/mesh/triangle_hash.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.8/kaolin/ops/mesh/triangle_hash.so\n",
            "building 'kaolin.ops.conversions.mise' extension\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/include/python3.8 -c kaolin/cython/ops/conversions/mise.cpp -o build/temp.linux-x86_64-3.8/kaolin/cython/ops/conversions/mise.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=mise -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "\u001b[01m\u001b[Kkaolin/cython/ops/conversions/mise.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KPyObject* __pyx_pf_6kaolin_3ops_11conversions_4mise_4MISE_8get_points(__pyx_obj_6kaolin_3ops_11conversions_4mise_MISE*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kkaolin/cython/ops/conversions/mise.cpp:3476:35:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "   for (__pyx_t_10 = 0; \u001b[01;35m\u001b[K__pyx_t_10 < __pyx_t_9\u001b[m\u001b[K; __pyx_t_10+=1) {\n",
            "                        \u001b[01;35m\u001b[K~~~~~~~~~~~^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kkaolin/cython/ops/conversions/mise.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid __pyx_f_6kaolin_3ops_11conversions_4mise_4MISE_subdivide_voxels(__pyx_obj_6kaolin_3ops_11conversions_4mise_MISE*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kkaolin/cython/ops/conversions/mise.cpp:3703:33:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "   for (__pyx_t_9 = 0; \u001b[01;35m\u001b[K__pyx_t_9 < __pyx_t_11\u001b[m\u001b[K; __pyx_t_9+=1) {\n",
            "                       \u001b[01;35m\u001b[K~~~~~~~~~~^~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kkaolin/cython/ops/conversions/mise.cpp:3712:62:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "     __pyx_t_12 = (((__pyx_v_self->voxels[__pyx_v_idx]).level == __pyx_v_self->depth) != 0);\n",
            "\u001b[01m\u001b[Kkaolin/cython/ops/conversions/mise.cpp:3744:33:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "   for (__pyx_t_9 = 0; \u001b[01;35m\u001b[K__pyx_t_9 < __pyx_t_11\u001b[m\u001b[K; __pyx_t_9+=1) {\n",
            "                       \u001b[01;35m\u001b[K~~~~~~~~~~^~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kkaolin/cython/ops/conversions/mise.cpp:3753:62:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "     __pyx_t_12 = (((__pyx_v_self->voxels[__pyx_v_idx]).level == __pyx_v_self->depth) != 0);\n",
            "\u001b[01m\u001b[Kkaolin/cython/ops/conversions/mise.cpp:\u001b[m\u001b[K At global scope:\n",
            "\u001b[01m\u001b[Kkaolin/cython/ops/conversions/mise.cpp:15782:18:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KPyObject* __pyx_convert__to_py_struct____pyx_t_6kaolin_3ops_11conversions_4mise_Vector3D(__pyx_t_6kaolin_3ops_11conversions_4mise_Vector3D)\u001b[m\u001b[K’ defined but not used [\u001b[01;35m\u001b[K-Wunused-function\u001b[m\u001b[K]\n",
            " static PyObject* \u001b[01;35m\u001b[K__pyx_convert__to_py_struct____pyx_t_6kaolin_3ops_11conversions_4mise_Vector3D\u001b[m\u001b[K(struct __pyx_t_6kaolin_3ops_11conversions_4mise_Vector3D s) {\n",
            "                  \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/kaolin/cython/ops/conversions/mise.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.8/kaolin/ops/conversions/mise.so\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/__init__.py -> /usr/local/lib/python3.8/dist-packages/kaolin\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/_C.so -> /usr/local/lib/python3.8/dist-packages/kaolin\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/ops\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/ops/random.py -> /usr/local/lib/python3.8/dist-packages/kaolin/ops\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/ops/coords.py -> /usr/local/lib/python3.8/dist-packages/kaolin/ops\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/ops/__init__.py -> /usr/local/lib/python3.8/dist-packages/kaolin/ops\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/ops/gcn.py -> /usr/local/lib/python3.8/dist-packages/kaolin/ops\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/ops/voxelgrid.py -> /usr/local/lib/python3.8/dist-packages/kaolin/ops\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/ops/reduction.py -> /usr/local/lib/python3.8/dist-packages/kaolin/ops\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/ops/batch.py -> /usr/local/lib/python3.8/dist-packages/kaolin/ops\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/ops/mesh\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/ops/mesh/trianglemesh.py -> /usr/local/lib/python3.8/dist-packages/kaolin/ops/mesh\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/ops/mesh/tetmesh.py -> /usr/local/lib/python3.8/dist-packages/kaolin/ops/mesh\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/ops/mesh/check_sign.py -> /usr/local/lib/python3.8/dist-packages/kaolin/ops/mesh\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/ops/mesh/mesh.py -> /usr/local/lib/python3.8/dist-packages/kaolin/ops/mesh\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/ops/mesh/__init__.py -> /usr/local/lib/python3.8/dist-packages/kaolin/ops/mesh\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/ops/mesh/triangle_hash.so -> /usr/local/lib/python3.8/dist-packages/kaolin/ops/mesh\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/ops/conversions\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/ops/conversions/tetmesh.py -> /usr/local/lib/python3.8/dist-packages/kaolin/ops/conversions\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/ops/conversions/voxelgrid.py -> /usr/local/lib/python3.8/dist-packages/kaolin/ops/conversions\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/ops/conversions/__init__.py -> /usr/local/lib/python3.8/dist-packages/kaolin/ops/conversions\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/ops/conversions/trianglemesh.py -> /usr/local/lib/python3.8/dist-packages/kaolin/ops/conversions\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/ops/conversions/pointcloud.py -> /usr/local/lib/python3.8/dist-packages/kaolin/ops/conversions\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/ops/conversions/sdf.py -> /usr/local/lib/python3.8/dist-packages/kaolin/ops/conversions\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/ops/conversions/mise.so -> /usr/local/lib/python3.8/dist-packages/kaolin/ops/conversions\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/ops/spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/ops/spc/convolution.py -> /usr/local/lib/python3.8/dist-packages/kaolin/ops/spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/ops/spc/points.py -> /usr/local/lib/python3.8/dist-packages/kaolin/ops/spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/ops/spc/spc.py -> /usr/local/lib/python3.8/dist-packages/kaolin/ops/spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/ops/spc/__init__.py -> /usr/local/lib/python3.8/dist-packages/kaolin/ops/spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/ops/spc/uint8.py -> /usr/local/lib/python3.8/dist-packages/kaolin/ops/spc\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/io\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/io/modelnet.py -> /usr/local/lib/python3.8/dist-packages/kaolin/io\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/io/obj.py -> /usr/local/lib/python3.8/dist-packages/kaolin/io\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/io/render.py -> /usr/local/lib/python3.8/dist-packages/kaolin/io\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/io/off.py -> /usr/local/lib/python3.8/dist-packages/kaolin/io\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/io/usd.py -> /usr/local/lib/python3.8/dist-packages/kaolin/io\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/io/__init__.py -> /usr/local/lib/python3.8/dist-packages/kaolin/io\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/io/dataset.py -> /usr/local/lib/python3.8/dist-packages/kaolin/io\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/io/utils.py -> /usr/local/lib/python3.8/dist-packages/kaolin/io\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/io/materials.py -> /usr/local/lib/python3.8/dist-packages/kaolin/io\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/io/shapenet.py -> /usr/local/lib/python3.8/dist-packages/kaolin/io\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/io/shrec.py -> /usr/local/lib/python3.8/dist-packages/kaolin/io\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/metrics\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/metrics/tetmesh.py -> /usr/local/lib/python3.8/dist-packages/kaolin/metrics\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/metrics/render.py -> /usr/local/lib/python3.8/dist-packages/kaolin/metrics\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/metrics/pointcloud.py -> /usr/local/lib/python3.8/dist-packages/kaolin/metrics\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/metrics/__init__.py -> /usr/local/lib/python3.8/dist-packages/kaolin/metrics\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/metrics/voxelgrid.py -> /usr/local/lib/python3.8/dist-packages/kaolin/metrics\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/metrics/trianglemesh.py -> /usr/local/lib/python3.8/dist-packages/kaolin/metrics\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/render\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/render/__init__.py -> /usr/local/lib/python3.8/dist-packages/kaolin/render\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/render/camera\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/render/camera/intrinsics_pinhole.py -> /usr/local/lib/python3.8/dist-packages/kaolin/render/camera\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/render/camera/extrinsics.py -> /usr/local/lib/python3.8/dist-packages/kaolin/render/camera\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/render/camera/intrinsics_ortho.py -> /usr/local/lib/python3.8/dist-packages/kaolin/render/camera\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/render/camera/coordinates.py -> /usr/local/lib/python3.8/dist-packages/kaolin/render/camera\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/render/camera/camera.py -> /usr/local/lib/python3.8/dist-packages/kaolin/render/camera\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/render/camera/__init__.py -> /usr/local/lib/python3.8/dist-packages/kaolin/render/camera\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/render/camera/legacy.py -> /usr/local/lib/python3.8/dist-packages/kaolin/render/camera\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/render/camera/intrinsics.py -> /usr/local/lib/python3.8/dist-packages/kaolin/render/camera\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/render/camera/extrinsics_backends.py -> /usr/local/lib/python3.8/dist-packages/kaolin/render/camera\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/render/mesh\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/render/mesh/dibr.py -> /usr/local/lib/python3.8/dist-packages/kaolin/render/mesh\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/render/mesh/utils.py -> /usr/local/lib/python3.8/dist-packages/kaolin/render/mesh\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/render/mesh/deftet.py -> /usr/local/lib/python3.8/dist-packages/kaolin/render/mesh\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/render/mesh/rasterization.py -> /usr/local/lib/python3.8/dist-packages/kaolin/render/mesh\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/render/mesh/__init__.py -> /usr/local/lib/python3.8/dist-packages/kaolin/render/mesh\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/render/spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/render/spc/__init__.py -> /usr/local/lib/python3.8/dist-packages/kaolin/render/spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/render/spc/raytrace.py -> /usr/local/lib/python3.8/dist-packages/kaolin/render/spc\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/rep\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/rep/spc.py -> /usr/local/lib/python3.8/dist-packages/kaolin/rep\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/rep/__init__.py -> /usr/local/lib/python3.8/dist-packages/kaolin/rep\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/utils\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/utils/testing.py -> /usr/local/lib/python3.8/dist-packages/kaolin/utils\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/utils/__init__.py -> /usr/local/lib/python3.8/dist-packages/kaolin/utils\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/visualize\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/visualize/timelapse.py -> /usr/local/lib/python3.8/dist-packages/kaolin/visualize\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/visualize/__init__.py -> /usr/local/lib/python3.8/dist-packages/kaolin/visualize\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/csrc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/packed_base.cuh -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/utils.h -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/spc_math.h -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/spc_utils.cuh -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/3d_math.cuh -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/check.h -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/2d_math.cuh -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/bindings.cpp -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/csrc/metrics\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/metrics/sided_distance.h -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/metrics\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/metrics/sided_distance_cuda.cu -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/metrics\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/metrics/unbatched_triangle_distance.h -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/metrics\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/metrics/sided_distance.cpp -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/metrics\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/metrics/unbatched_triangle_distance_cuda.cu -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/metrics\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/metrics/unbatched_triangle_distance.cpp -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/metrics\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/packed_simple_sum.cpp -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/tile_to_packed_cuda.cu -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/tile_to_packed.cpp -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/packed_simple_sum_cuda.cu -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/tile_to_packed.h -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/packed_simple_sum.h -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/conversions\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/conversions/mesh_to_spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/conversions/mesh_to_spc/mesh_to_spc_cuda.cu -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/conversions/mesh_to_spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/conversions/mesh_to_spc/mesh_to_spc.h -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/conversions/mesh_to_spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/conversions/mesh_to_spc/mesh_to_spc.cpp -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/conversions/mesh_to_spc\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/conversions/unbatched_mcube\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/conversions/unbatched_mcube/tables.h -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/conversions/unbatched_mcube\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/conversions/unbatched_mcube/unbatched_mcube_cuda.cu -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/conversions/unbatched_mcube\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/conversions/unbatched_mcube/helper_math.h -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/conversions/unbatched_mcube\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/conversions/unbatched_mcube/unbatched_mcube.cpp -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/conversions/unbatched_mcube\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/conversions/unbatched_mcube/defines.h -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/conversions/unbatched_mcube\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/conversions/unbatched_mcube/unbatched_mcube.h -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/conversions/unbatched_mcube\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/mesh\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/mesh/mesh_intersection.cpp -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/mesh\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/mesh/mesh_intersection.h -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/mesh\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/mesh/mesh_intersection_cuda.cu -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/mesh\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/spc/feature_grids.cpp -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/spc/point_utils.h -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/spc/spc.h -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/spc/scan_octrees.cu -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/spc/feature_grids_cuda.cu -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/spc/spc.cpp -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/spc/query_cuda.cu -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/spc/minkowski_conv.cu -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/spc/convolution_cuda.cu -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/spc/point_utils.cpp -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/spc/point_utils_cuda.cu -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/spc/convolution.cuh -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/spc/query.cpp -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/spc/generate_points.cu -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/spc/feature_grids.h -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/spc/convolution.cpp -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/ops/spc/query.h -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/ops/spc\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/csrc/render\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/csrc/render/mesh\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/render/mesh/rasterization.cpp -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/render/mesh\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/render/mesh/rasterization.h -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/render/mesh\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/render/mesh/deftet.h -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/render/mesh\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/render/mesh/deftet_cuda.cu -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/render/mesh\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/render/mesh/dibr_soft_mask_cuda.cu -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/render/mesh\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/render/mesh/dibr_soft_mask.h -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/render/mesh\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/render/mesh/rasterization_cuda.cu -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/render/mesh\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/render/mesh/deftet.cpp -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/render/mesh\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/render/mesh/dibr_soft_mask.cpp -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/render/mesh\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/csrc/render/spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/render/spc/raytrace_cuda.cu -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/render/spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/render/spc/raytrace.cpp -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/render/spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/render/spc/raytrace.h -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/render/spc\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/csrc/render/spc/spc_render_utils.cuh -> /usr/local/lib/python3.8/dist-packages/kaolin/csrc/render/spc\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/cython\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/cython/ops\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/cython/ops/conversions\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/cython/ops/conversions/mise.pyx -> /usr/local/lib/python3.8/dist-packages/kaolin/cython/ops/conversions\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/cython/ops/conversions/mise.cpp -> /usr/local/lib/python3.8/dist-packages/kaolin/cython/ops/conversions\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/cython/ops/mesh\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/cython/ops/mesh/triangle_hash.pyx -> /usr/local/lib/python3.8/dist-packages/kaolin/cython/ops/mesh\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/cython/ops/mesh/triangle_hash.cpp -> /usr/local/lib/python3.8/dist-packages/kaolin/cython/ops/mesh\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/experimental\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/experimental/dash3d/Makefile -> /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/experimental/dash3d/util.py -> /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/experimental/dash3d/README.md -> /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/experimental/dash3d/__init__.py -> /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/experimental/dash3d/kaolin-dash3d -> /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/experimental/dash3d/run.py -> /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d/src\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/experimental/dash3d/src/render.js -> /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d/src\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/experimental/dash3d/src/util.js -> /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d/src\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/experimental/dash3d/src/test_util.js -> /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d/src\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/experimental/dash3d/src/geometry.js -> /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d/src\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/experimental/dash3d/src/style.css -> /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d/src\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/experimental/dash3d/src/copyright.js -> /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d/src\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/experimental/dash3d/src/controller.js -> /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d/src\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d/static\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/experimental/dash3d/static/style.css -> /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d/static\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/experimental/dash3d/static/core-min.js -> /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d/static\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/experimental/dash3d/static/thirdparty.js -> /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d/static\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/experimental/dash3d/static/thirdparty.css -> /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d/static\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/experimental/dash3d/static/favicon.ico -> /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d/static\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/experimental/dash3d/static/green_plastic.frag -> /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d/static\n",
            "creating /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d/templates\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/experimental/dash3d/templates/view_settings.html -> /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d/templates\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/experimental/dash3d/templates/home.html -> /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d/templates\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/experimental/dash3d/templates/render_card.html -> /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d/templates\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/experimental/dash3d/templates/render_row.html -> /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d/templates\n",
            "copying build/lib.linux-x86_64-3.8/kaolin/version.py -> /usr/local/lib/python3.8/dist-packages/kaolin\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/ops/random.py to random.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/ops/coords.py to coords.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/ops/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/ops/gcn.py to gcn.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/ops/voxelgrid.py to voxelgrid.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/ops/reduction.py to reduction.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/ops/batch.py to batch.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/ops/mesh/trianglemesh.py to trianglemesh.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/ops/mesh/tetmesh.py to tetmesh.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/ops/mesh/check_sign.py to check_sign.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/ops/mesh/mesh.py to mesh.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/ops/mesh/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/ops/conversions/tetmesh.py to tetmesh.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/ops/conversions/voxelgrid.py to voxelgrid.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/ops/conversions/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/ops/conversions/trianglemesh.py to trianglemesh.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/ops/conversions/pointcloud.py to pointcloud.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/ops/conversions/sdf.py to sdf.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/ops/spc/convolution.py to convolution.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/ops/spc/points.py to points.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/ops/spc/spc.py to spc.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/ops/spc/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/ops/spc/uint8.py to uint8.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/io/modelnet.py to modelnet.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/io/obj.py to obj.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/io/render.py to render.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/io/off.py to off.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/io/usd.py to usd.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/io/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/io/dataset.py to dataset.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/io/utils.py to utils.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/io/materials.py to materials.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/io/shapenet.py to shapenet.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/io/shrec.py to shrec.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/metrics/tetmesh.py to tetmesh.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/metrics/render.py to render.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/metrics/pointcloud.py to pointcloud.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/metrics/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/metrics/voxelgrid.py to voxelgrid.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/metrics/trianglemesh.py to trianglemesh.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/render/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/render/camera/intrinsics_pinhole.py to intrinsics_pinhole.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/render/camera/extrinsics.py to extrinsics.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/render/camera/intrinsics_ortho.py to intrinsics_ortho.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/render/camera/coordinates.py to coordinates.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/render/camera/camera.py to camera.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/render/camera/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/render/camera/legacy.py to legacy.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/render/camera/intrinsics.py to intrinsics.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/render/camera/extrinsics_backends.py to extrinsics_backends.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/render/mesh/dibr.py to dibr.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/render/mesh/utils.py to utils.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/render/mesh/deftet.py to deftet.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/render/mesh/rasterization.py to rasterization.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/render/mesh/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/render/spc/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/render/spc/raytrace.py to raytrace.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/rep/spc.py to spc.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/rep/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/utils/testing.py to testing.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/utils/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/visualize/timelapse.py to timelapse.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/visualize/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d/util.py to util.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/experimental/dash3d/run.py to run.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/kaolin/version.py to version.cpython-38.pyc\n",
            "running install_scripts\n",
            "running build_scripts\n",
            "changing mode of build/scripts-3.8/kaolin-dash3d from 600 to 755\n",
            "copying build/scripts-3.8/kaolin-dash3d -> /usr/local/bin\n",
            "changing mode of /usr/local/bin/kaolin-dash3d to 755\n",
            "running build\n"
          ]
        }
      ],
      "source": [
        "!python setup.py install_lib install_scripts build\n",
        "# !python setup.py --help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-ItNJXfPQPA"
      },
      "source": [
        "# Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xI6uNv5JPP0i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b97fa17-0f7c-4af7-afe3-02cf342af3dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  404k    0  404k    0     0   383k      0 --:--:--  0:00:01 --:--:-- 15.7M\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/facebookresearch/pytorch3d.git@stable\n",
            "  Cloning https://github.com/facebookresearch/pytorch3d.git (to revision stable) to /tmp/pip-req-build-e924e2bm\n",
            "  Running command git clone -q https://github.com/facebookresearch/pytorch3d.git /tmp/pip-req-build-e924e2bm\n",
            "  Running command git checkout -q 995b60e3b99faa1ee1bcdbe244426d54d98a7242\n",
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221122.tar.gz (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting iopath\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fvcore->pytorch3d==0.7.1) (1.21.6)\n",
            "Collecting yacs>=0.1.6\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from fvcore->pytorch3d==0.7.1) (6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from fvcore->pytorch3d==0.7.1) (4.64.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.8/dist-packages (from fvcore->pytorch3d==0.7.1) (2.1.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from fvcore->pytorch3d==0.7.1) (7.1.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from fvcore->pytorch3d==0.7.1) (0.8.10)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.8/dist-packages (from iopath->pytorch3d==0.7.1) (4.4.0)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: pytorch3d, fvcore, iopath\n",
            "  Building wheel for pytorch3d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch3d: filename=pytorch3d-0.7.1-cp38-cp38-linux_x86_64.whl size=51455509 sha256=4948d8dc92c778b7e20f163c0f17ce7df798d734551383ffbdf26f31fa4bf584\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-g6hsao8s/wheels/db/6f/d4/ae9aeabc4d0ffaef2112aa0e541dd8a947c6008ae0866d5390\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221122-py3-none-any.whl size=61484 sha256=43163379adc55f53cc68053d5235989107ee537de156a11e1cc062dcf707e1c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/6e/e3/602889ca9c5c55020f8d205066445ac5b1b96df59f75170ca0\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31547 sha256=8ea4800aa50853a4a27075c0cf11182c76e7dd3e2475223a00acfeb14ada95fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/3e/24/0f349c0b2eeb6965903035f3b00dbb5c9bea437b4a2f18d82c\n",
            "Successfully built pytorch3d fvcore iopath\n",
            "Installing collected packages: portalocker, yacs, iopath, fvcore, pytorch3d\n",
            "Successfully installed fvcore-0.1.5.post20221122 iopath-0.1.10 portalocker-2.6.0 pytorch3d-0.7.1 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import kaolin\n",
        "import sys\n",
        "import os\n",
        "\n",
        "need_pytorch3d=False\n",
        "try:\n",
        "    import pytorch3d\n",
        "except ModuleNotFoundError:\n",
        "    need_pytorch3d=True\n",
        "if need_pytorch3d:\n",
        "    if torch.__version__.startswith(\"1.12.\") and sys.platform.startswith(\"linux\"):\n",
        "        # We try to install PyTorch3D via a released wheel.\n",
        "        pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
        "        version_str=\"\".join([\n",
        "            f\"py3{sys.version_info.minor}_cu\",\n",
        "            torch.version.cuda.replace(\".\",\"\"),\n",
        "            f\"_pyt{pyt_version_str}\"\n",
        "        ])\n",
        "        print(f\"version_str : {version_str}\")\n",
        "        !pip install fvcore iopath\n",
        "        !pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
        "    else:\n",
        "        # We try to install PyTorch3D from source.\n",
        "        !curl -LO https://github.com/NVIDIA/cub/archive/1.10.0.tar.gz\n",
        "        !tar xzf 1.10.0.tar.gz\n",
        "        os.environ[\"CUB_HOME\"] = os.getcwd() + \"/cub-1.10.0\"\n",
        "        !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'\n",
        "\n",
        "from kaolin.ops.conversions import (\n",
        "    trianglemeshes_to_voxelgrids,\n",
        "    marching_tetrahedra,\n",
        "    voxelgrids_to_cubic_meshes,\n",
        "    voxelgrids_to_trianglemeshes,\n",
        ")\n",
        "\n",
        "from kaolin.ops.mesh import (\n",
        "    index_vertices_by_faces\n",
        ")\n",
        "\n",
        "from kaolin.io.shapenet import (\n",
        "    ShapeNetV2\n",
        ")\n",
        "\n",
        "from kaolin.metrics.trianglemesh import (\n",
        "    point_to_mesh_distance,\n",
        "\n",
        ")\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# add path for demo utils functions \n",
        "sys.path.append(os.path.abspath(''))\n",
        "sys.path.append('/content/drive/MyDrive/CV_DMTet/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKhngPBlPbTs"
      },
      "source": [
        "# Import Dataset: Subset of ShapeNetV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LWdkJJr1y_wW"
      },
      "outputs": [],
      "source": [
        "import pytorch3d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "G126GQfpf1nB"
      },
      "outputs": [],
      "source": [
        "# state_dict = torch.load('/content/drive/MyDrive/CV_DMTet/shapenet.pvcnn.c1.pth.tar')\n",
        "# state_dict.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yMSLuRoDPfkp"
      },
      "outputs": [],
      "source": [
        "sys.path.append(INSTALL_PATH / \"examples\")\n",
        "sys.path.append(INSTALL_PATH / \"examples\" / \"tutorial\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aTCgE2WCPiAr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "544ba487-b67f-44eb-a648-c7fc57e3bc2f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    torch.cuda.set_device(device)\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "i3zs2M-kOA1z"
      },
      "outputs": [],
      "source": [
        "# path to the point cloud to be reconstructed\n",
        "pcd_path = KAOLIN_PATH / \"examples/samples/bear_pointcloud.usd\"\n",
        "# path to the output logs (readable with the training visualizer in the omniverse app)\n",
        "logs_path = '/content/drive/MyDrive/CV_DMTet/Logs'\n",
        "\n",
        "# We initialize the timelapse that will store USD for the visualization apps\n",
        "timelapse = kaolin.visualize.Timelapse(logs_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-D0gGzDS3-7"
      },
      "source": [
        "#Load Tetrahedral grid\n",
        "\n",
        "DMTet starts from a uniform tetrahedral grid of predefined resolution, and uses a network to predict the SDF value as well as deviation vector at each grid vertex.\n",
        "\n",
        "Here we load the pre-generated tetrahedral grid using Quartet at resolution 128, which has roughly the same number of vertices as a voxel grid of resolution 65. We use a simple MLP + positional encoding to predict the SDF and deviation vectors in DMTet, and initialize the encoded SDF to represent a sphere."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZbYIPn4OEuRV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oEEdkp9yFI2r"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LyIoND6-SxAD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee35a521-115b-4c7a-8a00-5cafd2b5b9f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.5000,  0.5000,  0.4844],\n",
            "        [ 0.4844,  0.5000,  0.4922],\n",
            "        [ 0.4922,  0.4844,  0.4844],\n",
            "        ...,\n",
            "        [-0.1719, -0.5000,  0.4766],\n",
            "        [-0.1562, -0.5000,  0.4688],\n",
            "        [-0.1562, -0.4922,  0.4688]], device='cuda:0') tensor([[     0,      1,      2,      3],\n",
            "        [     2,      3,      1,      4],\n",
            "        [     5,      3,      0,      2],\n",
            "        ...,\n",
            "        [277409, 272920, 272914, 272919],\n",
            "        [272919, 277409, 272920, 274866],\n",
            "        [277409, 277400, 272920, 274866]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-e89bff709631>:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
            "  tets = torch.tensor(([np.load(KAOLIN_PATH / 'examples/samples/128_tets_{}.npz'.format(i))['data'] for i in range(4)]), dtype=torch.long, device=device).permute(1,0)\n"
          ]
        }
      ],
      "source": [
        "# Uniform Tetrahedral Grid\n",
        "tets_verts = torch.tensor(np.load(KAOLIN_PATH / \"examples/samples/128_verts.npz\")['data'], dtype=torch.float, device=device)\n",
        "tets = torch.tensor(([np.load(KAOLIN_PATH / 'examples/samples/128_tets_{}.npz'.format(i))['data'] for i in range(4)]), dtype=torch.long, device=device).permute(1,0)\n",
        "print(tets_verts, tets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LTFQbA5awlr"
      },
      "source": [
        "# Loading from ShapeNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fxK5Qw4xay7g"
      },
      "outputs": [],
      "source": [
        "# Not using for now, using libigl tutorial x-cylinder below\n",
        "# SHAPENET_PATH = \"/content/drive/MyDrive/CV_DMTet/Core\"\n",
        "# #SHAPENET_PATH = \"/content/drive/MyDrive/FALL 2022/Computer Vision/Project/Core\"\n",
        "# SYNSETS_IDS = ['02747177', '02801938', '02808440', '02818832', '02828884', '02871439', '02876657', '02880940', '02924116', '02933112']\n",
        "# shapenet_train = ShapeNetV2(SHAPENET_PATH, categories=SYNSETS_IDS, output_dict=True)\n",
        "# shapenet_test = ShapeNetV2(SHAPENET_PATH, categories=SYNSETS_IDS, output_dict=True, train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uhzpP8kua2Rm"
      },
      "outputs": [],
      "source": [
        "# sample_model = shapenet_train[3] # change the index here for different models\n",
        "# sample_verts = sample_model['mesh'][0]\n",
        "# sample_faces = sample_model['mesh'][1]\n",
        "\n",
        "# timelapse.add_mesh_batch(\n",
        "#     category='gt',\n",
        "#     vertices_list=[sample_verts.cpu()],\n",
        "#     faces_list=[sample_faces.cpu()]\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RKRJZwFjPaPl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "K8Cm7qVUA_hl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "684e91c2-77f3-4d2e-d600-66c1445f6e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LIBIGL_TUTORIAL_DATA: /content/drive/MyDrive/CV_DMTet/libigl-tutorial-data\n",
            "/content/drive/MyDrive/CV_DMTet\n",
            "/content/drive/MyDrive/CV_DMTet/kaolin\n"
          ]
        }
      ],
      "source": [
        "# Clone sample meshes from libigl tutorial 1x\n",
        "LIBIGL_TUTORIAL_DATA = INSTALL_PATH / \"libigl-tutorial-data\"\n",
        "print(f\"LIBIGL_TUTORIAL_DATA: {LIBIGL_TUTORIAL_DATA}\")\n",
        "%cd $INSTALL_PATH\n",
        "!if [ ! -d $LIBIGL_TUTORIAL_DATA ]; then git clone --recursive https://github.com/libigl/libigl-tutorial-data.git; fi;\n",
        "%cd $KAOLIN_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Wo9-f58yGLmK"
      },
      "outputs": [],
      "source": [
        "# Load cylinder (along x-axis) mesh (V,F) = (42,80)\n",
        "mesh = kaolin.io.obj.import_mesh(LIBIGL_TUTORIAL_DATA / \"xcylinder.obj\") #xcylinder.obj\n",
        "mesh_verts = mesh[0]\n",
        "mesh_faces = mesh[1]\n",
        "timelapse.add_mesh_batch(\n",
        "    category='gt',\n",
        "    vertices_list=[mesh_verts.cpu()],\n",
        "    faces_list=[mesh_faces.cpu()]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HjBnktJULzWd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXJG7i2wbHC7"
      },
      "source": [
        "# Convert model to watertight meshes\n",
        "\n",
        "We used a voxelization with resolution of 64 to predict the sdf and extract surface. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "e93m-kdBHD7G"
      },
      "outputs": [],
      "source": [
        "# wt_grid\n",
        "# Convert mesh (V,F) to a voxel grid \n",
        "voxels = kaolin.ops.conversions.trianglemeshes_to_voxelgrids(\n",
        "    vertices=mesh_verts.unsqueeze(0).to(device),\n",
        "    faces=mesh_faces.to(device),\n",
        "    resolution=16\n",
        ")\n",
        "\n",
        "# Convert the voxel grid back into a mesh as GroundTruth\n",
        "wt_verts, wt_faces = kaolin.ops.conversions.voxelgrids_to_cubic_meshes(voxels)\n",
        "wt_verts, wt_faces = wt_verts[0], wt_faces[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1-iQiev1XYcD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pcd_path = KAOLIN_PATH / \"examples/samples/bear_pointcloud.usd\"\n",
        "points = kaolin.io.usd.import_pointclouds(str(pcd_path))[0].points.to(device)\n",
        "if points.shape[0] > 100000:\n",
        "    idx = list(range(points.shape[0]))\n",
        "    np.random.shuffle(idx)\n",
        "    idx = torch.tensor(idx[:100000], device=points.device, dtype=torch.long)    \n",
        "    points = points[idx]\n",
        "\n",
        "# The reconstructed object needs to be slightly smaller than the grid to get watertight surface after MT.\n",
        "center = (points.max(0)[0] + points.min(0)[0]) / 2\n",
        "max_l = (points.max(0)[0] - points.min(0)[0]).max()\n",
        "points = ((points - center) / max_l)* 0.9"
      ],
      "metadata": {
        "id": "LfFGWqUYTPqx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timelapse.add_pointcloud_batch(category='input',\n",
        "                               pointcloud_list=[points.cpu()], points_type = \"usd_geom_points\")"
      ],
      "metadata": {
        "id": "DEBw5esdYX__"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8qo7DnerK7k2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "kaolin.ops.conversions.voxelgrids_to_cubic_meshes(\n",
        "\n",
        "Convert voxelgrids to meshes by replacing each occupied voxel with a cuboid mesh (unit cube). Each cube has 8 vertices and 6 (for quadmesh) or 12 faces (for triangular mesh). Internal faces are ignored. If is_trimesh==True, this function performs the same operation as “Cubify” defined in the ICCV 2019 paper “Mesh R-CNN”: https://arxiv.org/abs/1906.02739.\n",
        "\n",
        "Parameters\n",
        "voxelgrids (torch.Tensor) – binary voxel array, of shape .\n",
        "\n",
        "is_trimesh (optional, bool) – if True, the outputs are triangular meshes. Otherwise quadmeshes are returned. Default: True.\n",
        "\n",
        "Returns\n",
        "The list of vertices for each mesh.\n",
        "\n",
        "The list of faces for each mesh.\n",
        "\n",
        "Return type\n",
        "(list[torch.Tensor], list[torch.LongTensor])"
      ],
      "metadata": {
        "id": "D5X80R95XgJC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "SeRiCeTULDEv"
      },
      "outputs": [],
      "source": [
        "# wt_verts = kaolin.metrics.trianglemesh.uniform_laplacian_smoothing(wt_verts[0].unsqueeze(0), wt_faces[0])\n",
        "# sample_verts, sample_faces = wt_verts[0], wt_faces[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_gt_len = (mesh_verts.max(0)[0] - mesh_faces.min(0)[0]).max()\n",
        "max_vox_len = (wt_verts.max(0)[0] - wt_verts.min(0)[0]).max()\n",
        "scale = max_gt_len / max_vox_len"
      ],
      "metadata": {
        "id": "VUbuf5PETHwp"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "yO0vb_YKL5ya"
      },
      "outputs": [],
      "source": [
        "center = (wt_verts.max(0)[0] + wt_verts.min(0)[0]) / 2\n",
        "wt_verts = ((wt_verts - center) * scale)\n",
        "timelapse.add_mesh_batch(\n",
        "    category='watertight_test',\n",
        "    vertices_list=[wt_verts.cpu()],\n",
        "    faces_list=[wt_faces.cpu()]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Mc2UjaWTYqc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "kaolin.ops.conversions.marching_tetrahedra(vertices, tets, sdf, return_tet_idx=False)¶\n",
        "Convert discrete signed distance fields encoded on tetrahedral grids to triangle meshes using marching tetrahedra algorithm as described in An efficient method of triangulating equi-valued surfaces by using tetrahedral cells. The output surface is differentiable with respect to input vertex positions and the SDF values. For more details and example usage in learning, see Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis NeurIPS 2021.\n"
      ],
      "metadata": {
        "id": "BMbohHY-W8a_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scLBY-eAaBUf"
      },
      "source": [
        "# SDF model\n",
        "\n",
        "We follow the paper recommandation and use a four-layer\n",
        "MLPs with hidden dimensions 256, 256, 128 and 64, respectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ZnLJv65eKNGK"
      },
      "outputs": [],
      "source": [
        "# Since we skip PVCNN, input dimension is just the coordinates of each grid\n",
        "\n",
        "SDF_MLP_CONFIG = {\n",
        "    'input_dim' : 3, # Coordinates of the grid's vertices\n",
        "    'hidden_dims' : [256, 256, 128, 64],\n",
        "    'output_dim' : 1, # SDF of the vertex input. The other \"output\" f_v comes from the prior activation layer of dimension 64\n",
        "    'multires': 2\n",
        "}\n",
        "\n",
        "lr = 0.001\n",
        "laplacian_weight = 0.1\n",
        "iterations = 2000\n",
        "save_every = 100\n",
        "multires = 2\n",
        "grid_res = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "cS9kStkdKQlC"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "\n",
        "# MLP + Positional Encoding\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, input_dims = 3, internal_dims = 128, output_dims = 4, hidden = 5, multires = 2):\n",
        "        super().__init__()\n",
        "        self.embed_fn = None\n",
        "        if multires > 0:\n",
        "            embed_fn, input_ch = get_embedder(multires)\n",
        "            self.embed_fn = embed_fn\n",
        "            input_dims = input_ch\n",
        "\n",
        "        net = (torch.nn.Linear(input_dims, internal_dims, bias=False), torch.nn.ReLU())\n",
        "        for i in range(hidden-1):\n",
        "            net = net + (torch.nn.Linear(internal_dims, internal_dims, bias=False), torch.nn.ReLU())\n",
        "        net = net + (torch.nn.Linear(internal_dims, output_dims, bias=False),)\n",
        "        self.net = torch.nn.Sequential(*net)\n",
        "\n",
        "    def forward(self, p):\n",
        "        if self.embed_fn is not None:\n",
        "            p = self.embed_fn(p)\n",
        "        out = self.net(p)\n",
        "        return out\n",
        "\n",
        "    def pre_train_sphere(self, iter):\n",
        "        print (\"Initialize SDF to sphere\")\n",
        "        loss_fn = torch.nn.MSELoss()\n",
        "        optimizer = torch.optim.Adam(list(self.parameters()), lr=1e-4)\n",
        "        \n",
        "\n",
        "        for i in tqdm(range(iter)):\n",
        "            p = torch.rand((1024,3), device='cuda') - 0.5\n",
        "            ref_value  = torch.sqrt((p**2).sum(-1)) - 0.3\n",
        "            output = self(p)\n",
        "            loss = loss_fn(output[...,0], ref_value)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Pre-trained MLP\", loss.item())\n",
        "\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.input_dim = config['input_dim']\n",
        "        self.hidden_dims  = config['hidden_dims']\n",
        "        self.output_dim = config['output_dim']\n",
        "        self.multires = config['multires']\n",
        "\n",
        "        self.embed_fn = None\n",
        "        # if self.multires > 0:\n",
        "        #     embed_fn, input_ch = get_embedder(self.multires)\n",
        "        #     self.embed_fn = embed_fn\n",
        "        #     self.input_dim = input_ch\n",
        "\n",
        "        # Hidden layers\n",
        "        self.hiddens = nn.ModuleList()\n",
        "        in_dim = self.input_dim\n",
        "        for k in range(len(self.hidden_dims)):\n",
        "            self.hiddens.append(nn.Linear(in_dim, self.hidden_dims[k]))\n",
        "            in_dim = self.hidden_dims[k]\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = torch.nn.Linear(self.hidden_dims[-1], self.output_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.embed_fn is not None:\n",
        "            x = self.embed_fn(x)\n",
        "        for hidden in self.hiddens :\n",
        "            x = F.relu(hidden(x))\n",
        "        output = self.output_layer(x) # No activation (linear) cuz we do regression\n",
        "\n",
        "        return output, x # Return sdf predicted + f_v feature vector\n",
        "\n",
        "\n",
        "# Positional Encoding from https://github.com/yenchenlin/nerf-pytorch/blob/1f064835d2cca26e4df2d7d130daa39a8cee1795/run_nerf_helpers.py\n",
        "class Embedder:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.kwargs = kwargs\n",
        "        self.create_embedding_fn()\n",
        "        \n",
        "    def create_embedding_fn(self):\n",
        "        embed_fns = []\n",
        "        d = self.kwargs['input_dims']\n",
        "        out_dim = 0\n",
        "        if self.kwargs['include_input']:\n",
        "            embed_fns.append(lambda x : x)\n",
        "            out_dim += d\n",
        "            \n",
        "        max_freq = self.kwargs['max_freq_log2']\n",
        "        N_freqs = self.kwargs['num_freqs']\n",
        "        \n",
        "        if self.kwargs['log_sampling']:\n",
        "            freq_bands = 2.**torch.linspace(0., max_freq, steps=N_freqs)\n",
        "        else:\n",
        "            freq_bands = torch.linspace(2.**0., 2.**max_freq, steps=N_freqs)\n",
        "            \n",
        "        for freq in freq_bands:\n",
        "            for p_fn in self.kwargs['periodic_fns']:\n",
        "                embed_fns.append(lambda x, p_fn=p_fn, freq=freq : p_fn(x * freq))\n",
        "                out_dim += d\n",
        "                    \n",
        "        self.embed_fns = embed_fns\n",
        "        self.out_dim = out_dim\n",
        "        \n",
        "    def embed(self, inputs):\n",
        "        return torch.cat([fn(inputs) for fn in self.embed_fns], -1)\n",
        "\n",
        "def get_embedder(multires):\n",
        "    embed_kwargs = {\n",
        "                'include_input' : True,\n",
        "                'input_dims' : 3,\n",
        "                'max_freq_log2' : multires-1,\n",
        "                'num_freqs' : multires,\n",
        "                'log_sampling' : True,\n",
        "                'periodic_fns' : [torch.sin, torch.cos],\n",
        "    }\n",
        "    \n",
        "    embedder_obj = Embedder(**embed_kwargs)\n",
        "    embed = lambda x, eo=embedder_obj : eo.embed(x)\n",
        "    return embed, embedder_obj.out_dim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "sdf_model = Decoder(multires=2).to(device)\n",
        "sdf_model.pre_train_sphere(1000)\n",
        "# print(sdf_model)\n",
        "# print('\\n\\n')\n",
        "# summary(sdf_model, input_size=tets_verts.shape)"
      ],
      "metadata": {
        "id": "m_z6PaowshTB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3597179-2b4d-4b17-dfdd-a7f2f45aff55"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialize SDF to sphere\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:03<00:00, 317.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-trained MLP 6.868880063848337e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "KLOS3kFWrd4n"
      },
      "outputs": [],
      "source": [
        "# sdf_model = MLP(SDF_MLP_CONFIG).to(device)\n",
        "# print(sdf_model)\n",
        "# print('\\n\\n')\n",
        "# summary(sdf_model, input_size=tets_verts.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3RhnpiEL35m"
      },
      "source": [
        "Little Test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "SqcbsCj7L5eO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3c82ea9-f3bb-4276-d74b-2f319a9ae40b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input grid shape is : torch.Size([277410, 3])\n"
          ]
        }
      ],
      "source": [
        "# signed distance fields encoded on tetrahedral grid\n",
        "\n",
        "# pred_sdfs dim = (tets_vertices.shape[0])\n",
        "# f_vs ie f_v feature vector\n",
        "\n",
        "# sdf, delta x_i, delta y_i, delta z_i, \n",
        "# sdf = binary occupancy of tetrahedron \n",
        "pred = sdf_model(tets_verts)\n",
        "\n",
        "# pred_sdfs, f_vs = sdf_model(tets_verts)\n",
        "\n",
        "print(f'Input grid shape is : {tets_verts.shape}')\n",
        "# print(f'Output shape of the predicted SDFs should be {tets_verts.shape[0], 1} and it actually is {tuple(pred_sdfs.shape)}')\n",
        "# print(f'Output shape of the feature vectors f_vs should be {tets_verts.shape[0], 64} and it actually is {tuple(f_vs.shape)}')\n",
        "# print(pred_sdfs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WeFlWiqDJrPK"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Model from DMTet Tutorial"
      ],
      "metadata": {
        "id": "2_4nDFUooHT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Initialize model and create optimizer\n",
        "sdf_model = Decoder(multires=2).to(device)\n",
        "sdf_model.pre_train_sphere(1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeX6jmo_oERf",
        "outputId": "e9acc6be-543a-4609-85fd-d9b9863e4227"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialize SDF to sphere\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:02<00:00, 414.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-trained MLP 5.597603376372717e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_Dh9gJiTWEq"
      },
      "source": [
        "# Set up Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "rENv3tMyTSdT"
      },
      "outputs": [],
      "source": [
        "lr = 0.001\n",
        "sdf_vars = [p for _, p in sdf_model.named_parameters()]\n",
        "sdf_optimizer = torch.optim.Adam(sdf_vars, lr=lr)\n",
        "sdf_scheduler = torch.optim.lr_scheduler.LambdaLR(sdf_optimizer, lr_lambda=lambda x: max(0.0, 10**(-x*0.0002))) # LR decay over time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf6lBBcPTcwd"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "cDjptlAn_Biz"
      },
      "outputs": [],
      "source": [
        "# takes in a module and applies the specified weight initialization\n",
        "def weights_init_normal(m):\n",
        "    '''Takes in a module and initializes all linear layers with weight\n",
        "        values taken from a normal distribution.'''\n",
        "\n",
        "    classname = m.__class__.__name__\n",
        "    # for every Linear layer in a model\n",
        "    if classname.find('Linear') != -1:\n",
        "        y = m.in_features\n",
        "    # m.weight.data shoud be taken from a normal distribution\n",
        "        m.weight.data.normal_(0.0,1/np.sqrt(y))\n",
        "    # m.bias.data should be 0\n",
        "        m.bias.data.fill_(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "5qoVPBuJ_EaA"
      },
      "outputs": [],
      "source": [
        "# sdf_model.apply(weights_init_normal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "oSbqopQVh3yf"
      },
      "outputs": [],
      "source": [
        "# Laplacian regularization using umbrella operator (Fujiwara / Desbrun).\n",
        "# https://mgarland.org/class/geom04/material/smoothing.pdf\n",
        "def laplace_regularizer_const(pred_mesh_verts, pred_mesh_faces):\n",
        "    term = torch.zeros_like(pred_mesh_verts)\n",
        "    norm = torch.zeros_like(pred_mesh_verts[..., 0:1])\n",
        "\n",
        "    v0 = pred_mesh_verts[pred_mesh_faces[:, 0], :]\n",
        "    v1 = pred_mesh_verts[pred_mesh_faces[:, 1], :]\n",
        "    v2 = pred_mesh_verts[pred_mesh_faces[:, 2], :]\n",
        "\n",
        "    term.scatter_add_(0, pred_mesh_faces[:, 0:1].repeat(1,3), (v1 - v0) + (v2 - v0))\n",
        "    term.scatter_add_(0, pred_mesh_faces[:, 1:2].repeat(1,3), (v0 - v1) + (v2 - v1))\n",
        "    term.scatter_add_(0, pred_mesh_faces[:, 2:3].repeat(1,3), (v0 - v2) + (v1 - v2))\n",
        "\n",
        "    two = torch.ones_like(v0) * 2.0\n",
        "    norm.scatter_add_(0, pred_mesh_faces[:, 0:1], two)\n",
        "    norm.scatter_add_(0, pred_mesh_faces[:, 1:2], two)\n",
        "    norm.scatter_add_(0, pred_mesh_faces[:, 2:3], two)\n",
        "\n",
        "    term = term / torch.clamp(norm, min=1.0)\n",
        "\n",
        "    return torch.mean(term**2)\n",
        "\n",
        "def loss_f(pred_mesh_verts, pred_mesh_faces, points, it):\n",
        "    # sample_number = mesh_verts.shape[0] * .5\n",
        "    pred_points = kaolin.ops.mesh.sample_points(pred_mesh_verts.unsqueeze(0), pred_mesh_faces, 50000)[0][0]\n",
        "    chamfer = kaolin.metrics.pointcloud.chamfer_distance(pred_points.unsqueeze(0), points.unsqueeze(0)).mean()\n",
        "    # chamfer = torch.tensor(chamfer, requires_grad=True)\n",
        "    chamfer.clone().detach().requires_grad_(True)\n",
        "    if it > iterations//2:\n",
        "        lap = laplace_regularizer_const(pred_mesh_verts, pred_mesh_faces)\n",
        "        return chamfer + lap * laplacian_weight\n",
        "    return chamfer\n",
        "\n",
        "\n",
        "def sdf_train(iterations, model, optimizer, scheduler):\n",
        "  # Set to training mode\n",
        "  model.train()\n",
        "  \n",
        "  # # GT is the watertight mesh from\n",
        "  # gt_verts = wt_verts.to(device)\n",
        "  # gt_faces = wt_faces.to(device)\n",
        "\n",
        "  # f = index_vertices_by_faces(gt_verts.unsqueeze(0), gt_faces)\n",
        "  # gt_sdfs, idx, t = point_to_mesh_distance(tets_verts.unsqueeze(0), f)\n",
        "\n",
        "  # sign = kaolin.ops.mesh.check_sign(gt_verts.unsqueeze(0).to(device), gt_faces.to(device), tets_verts.unsqueeze(0))\n",
        "  # gt_sdfs[sign==False] *= -1\n",
        "\n",
        "  for it in range(iterations):\n",
        "      # pred_sdfs, f_vs = model(tets_verts)\n",
        "      # pred_sdfs = pred_sdfs.view(1,-1)\n",
        "\n",
        "      pred = model(tets_verts)\n",
        "      pred_sdfs, deform = pred[:,0], pred[:,1:]\n",
        "      verts_deformed = tets_verts + torch.tanh(deform) / grid_res # constraint deformation to avoid flipping tets\n",
        "      \n",
        "\n",
        "      pred_mesh_verts, pred_mesh_faces = marching_tetrahedra(verts_deformed.unsqueeze(0), tets, pred_sdfs.unsqueeze(0))\n",
        "      pred_mesh_verts, pred_mesh_faces = pred_mesh_verts[0], pred_mesh_faces[0]\n",
        "      # print(f\"mesh_verts: {mesh_verts.shape}\")\n",
        "      # print(f\"mesh_faces: {mesh_faces.shape}\")\n",
        "      \n",
        "      # print(pred_sdfs.shape)\n",
        "      loss = loss_f(pred_mesh_verts, pred_mesh_faces, points, it)\n",
        "      # print(loss.shape)\n",
        "      # loss = F.mse_loss(pred_sdfs, gt_sdfs.squeeze(0).unsqueeze(1), reduction='sum')\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "\n",
        "      if (it) % save_every == 0 or it == (iterations - 1): \n",
        "          print ('Iteration {} - loss: {}'.format(it, loss))\n",
        "          # save reconstructed mesh\n",
        "          timelapse.add_mesh_batch(\n",
        "              iteration=it+1,\n",
        "              category='extracted_mesh',\n",
        "              vertices_list=[pred_mesh_verts.cpu()],\n",
        "              faces_list=[pred_mesh_faces.cpu()]\n",
        "          )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pred_sdfs, f_vs = sdf_model(tets_verts)"
      ],
      "metadata": {
        "id": "5bFm72fRWCyG"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "MOLR1SY1IoFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b397606f-8f9f-4131-a883-d4c1e6d406d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0 - loss: 0.025023894384503365\n",
            "Iteration 100 - loss: 0.0021914495155215263\n",
            "Iteration 200 - loss: 0.0010193721391260624\n",
            "Iteration 300 - loss: 0.0002473253116477281\n",
            "Iteration 400 - loss: 0.00016402047185692936\n",
            "Iteration 500 - loss: 0.00031356397084891796\n",
            "Iteration 600 - loss: 0.00011820504732895643\n",
            "Iteration 700 - loss: 7.84399890108034e-05\n",
            "Iteration 800 - loss: 6.048430077498779e-05\n",
            "Iteration 900 - loss: 5.305161903379485e-05\n",
            "Iteration 1000 - loss: 5.10087629663758e-05\n",
            "Iteration 1100 - loss: 5.039621828473173e-05\n",
            "Iteration 1200 - loss: 4.737406561616808e-05\n",
            "Iteration 1300 - loss: 4.567881114780903e-05\n",
            "Iteration 1400 - loss: 4.4801210606237873e-05\n",
            "Iteration 1500 - loss: 4.4101965613663197e-05\n",
            "Iteration 1600 - loss: 4.311802695156075e-05\n",
            "Iteration 1700 - loss: 4.258201806806028e-05\n",
            "Iteration 1800 - loss: 4.207490565022454e-05\n",
            "Iteration 1900 - loss: 4.2618725274223834e-05\n",
            "Iteration 1999 - loss: 4.1919734940165654e-05\n"
          ]
        }
      ],
      "source": [
        "# ~12 min. Speed up?\n",
        "sdf_train(iterations, sdf_model, sdf_optimizer, sdf_scheduler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIZbLWiqaL5d"
      },
      "source": [
        " # Surface refinement utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMTthXJVeon8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Get edges lists of shape [E, 2] from face list of shape [V, 4]\n",
        "\n",
        "def get_edges(input):\n",
        "  c = torch.combinations(torch.arange(input.size(1)), r=2)\n",
        "  x = input[:,None].expand(-1,len(c),-1).cpu()\n",
        "  idx = c[None].expand(len(x), -1, -1)\n",
        "  x = x.gather(dim=2, index=idx)\n",
        "\n",
        "  return x.view(-1, *x.shape[2:])\n",
        "\n",
        "# Extract tets under certain sdf restrictions:\n",
        "# if thresh = 0, return all surface tetrahedrons\n",
        "# if thresh > 0, return all tetrahedrons whose vertices' sdfs are all in the range [-thresh, thresh]\n",
        "\n",
        "def extract_tet(tets, sdf, thresh, non_surf=False):\n",
        "\n",
        "  assert thresh >= 0\n",
        "\n",
        "  if thresh == 0:\n",
        "    mask = sdf[tets] > 0\n",
        "    mask_int = mask.squeeze(2).long()\n",
        "    t = mask_int.sum(1)\n",
        "    surf_tets = tets[(t > 0) & (t < 4)]\n",
        "  else:\n",
        "    mask = (sdf[tets] >= -thresh) & (sdf[tets] <= thresh)\n",
        "    mask_int = mask.squeeze(2).long()\n",
        "    t = mask_int.sum(1)\n",
        "    surf_tets = tets[t == 4]\n",
        "\n",
        "  surf_tets_tuple = surf_tets.unique(return_inverse=True)\n",
        "  surf_tets_idx, surf_tets = surf_tets_tuple[0], surf_tets_tuple[1]\n",
        "\n",
        "  if non_surf:\n",
        "    non_surf_tets = tets[t == 0]\n",
        "    non_surf_tets_tuple = non_surf_tets.unique(return_inverse=True)\n",
        "    non_surf_tets_idx, non_surf_tets = non_surf_tets_tuple[0], non_surf_tets_tuple[1]\n",
        "    return surf_tets_idx, surf_tets, non_surf_tets_idx, non_surf_tets\n",
        "\n",
        "  return surf_tets_idx, surf_tets\n",
        "\n",
        "#Get output from initial MLP\n",
        "\n",
        "def get_pred_sdfs(model, tets_verts):\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    pred_sdfs, f_vs = model(tets_verts)\n",
        "\n",
        "  return pred_sdfs, f_vs\n",
        "\n",
        "# Get ground truth sdf from input verts and faces\n",
        "# input shape: [batch_size, num_vertices, 3], [num_faces, 4], [batch_size, num_points, 3]\n",
        "# output shape: [num_points, 1]\n",
        "\n",
        "def get_gt_sdfs(gt_verts, gt_faces, points, f=None):\n",
        "\n",
        "  if f == None:\n",
        "    f = index_vertices_by_faces(gt_verts, gt_faces)\n",
        "  sdf,_,_ = point_to_mesh_distance(points, f)\n",
        "\n",
        "  s = kaolin.ops.mesh.check_sign(gt_verts, gt_faces, points)\n",
        "  sdf = sdf[s == True]\n",
        "\n",
        "  return sdf.squeeze(0).unsqueeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2K3Oyhenh3FO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfm6rGQnb3CG"
      },
      "source": [
        "# Check quality of sdf prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICl3ftwOb_YT"
      },
      "outputs": [],
      "source": [
        "# sdf = get_gt_sdfs(wt_verts.unsqueeze(0).to(device), wt_faces.to(device), tets_verts.unsqueeze(0).to(device))\n",
        "sdf,_ = get_pred_sdfs(sdf_model, tets_verts.unsqueeze(0).to(device))\n",
        "sdf = sdf.view(1, -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wtUjqe94mHIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sdf.shape)\n",
        "print(tets_verts.unsqueeze(0).shape)\n",
        "print(tets.shape)"
      ],
      "metadata": {
        "id": "k3kVToHPdVo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using marching tetrahedra to convert to \n",
        "verts_list, faces_list, tet_idx_list = marching_tetrahedra(tets_verts.unsqueeze(0).cpu(), tets.cpu(), sdf.cpu(), True)\n",
        "\n",
        "timelapse.add_mesh_batch(\n",
        "            category='marching_tet_surf',\n",
        "            vertices_list=verts_list,\n",
        "            faces_list=faces_list\n",
        "        )"
      ],
      "metadata": {
        "id": "oo1Ym5Giqxtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xUpAqjgGezoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sdf.shape)"
      ],
      "metadata": {
        "id": "DN46du7ve95L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqID6g79ccNN"
      },
      "outputs": [],
      "source": [
        "print(sdf[sdf > 0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nAc-DFKb5rd"
      },
      "outputs": [],
      "source": [
        "surf_tets_verts_idx, surf_tets_faces = extract_tet(tets, sdf, 0)\n",
        "surf_tets_verts = tets_verts[surf_tets_verts_idx]\n",
        "\n",
        "timelapse.add_mesh_batch(\n",
        "            category='tet_surface',\n",
        "            vertices_list=[surf_tets_verts.cpu()],\n",
        "            faces_list=[surf_tets_faces.cpu()]\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrEgkw5TksFD"
      },
      "source": [
        "# Surface refinement model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9uLf6MBUn49"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Graph-res net:\n",
        "Identify surface tetrahedral, build adj matrix, \n",
        "\"\"\"\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from pytorch3d.ops import GraphConv\n",
        "\n",
        "# a single res block layer with dimension 256 & 128\n",
        "class GResBlock(nn.Module): \n",
        "    def __init__(self, in_dim, hidden_dim, activation=None):\n",
        "        super(GResBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = GraphConv(in_dim, hidden_dim)\n",
        "        self.conv2 = GraphConv(hidden_dim, in_dim)\n",
        "        self.activation = F.relu if activation else None\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        input, adj = inputs[0], inputs[1]\n",
        "        x = self.conv1(input, adj)\n",
        "        if self.activation:\n",
        "          x = self.activation(x)\n",
        "        x = self.conv2(x, adj)\n",
        "        if self.activation:\n",
        "          x = self.activation(input + x)\n",
        "        \n",
        "        return [x, adj]\n",
        "\n",
        "class GBottleneck(nn.Module):\n",
        "    def __init__(self, block_num, in_dim, hidden_dim, out_dim, activation=None):\n",
        "        super(GBottleneck, self).__init__()\n",
        "\n",
        "        resblock_layers = [GResBlock(in_dim=hidden_dim[0], hidden_dim=hidden_dim[1], activation=activation)\n",
        "                          for _ in range(block_num)]\n",
        "        self.blocks = nn.Sequential(*resblock_layers)\n",
        "        self.conv1 = GraphConv(in_dim, hidden_dim[0])\n",
        "\n",
        "        self.activation = F.relu if activation else None\n",
        "    \n",
        "    def forward(self, inputs, adj):\n",
        "        x = self.conv1(inputs, adj)\n",
        "        if self.activation:\n",
        "          x = self.activation(x)\n",
        "        x = self.blocks([x, adj])[0]\n",
        "        if self.activation:\n",
        "          x = self.activation(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class GCN_Res(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(GCN_Res, self).__init__()\n",
        "\n",
        "        self.in_dim = config['in_dim']\n",
        "        self.hidden_dim = config['hidden_dim']\n",
        "        self.out_dim = config['out_dim']\n",
        "        self.activation = config['activation']\n",
        "        self.mlp_hdim = config['mlp_hdim']\n",
        "        self.mlp_odim = config['mlp_odim']\n",
        "\n",
        "        self.gcn_res = nn.ModuleList([GBottleneck(2, self.in_dim, self.hidden_dim, self.out_dim, self.activation)])\n",
        "\n",
        "        self.sdf_mlp = nn.Sequential(\n",
        "            nn.Linear(self.out_dim, self.mlp_hdim[0], bias=False),\n",
        "            nn.Linear(self.mlp_hdim[0], self.mlp_hdim[1], bias=False),\n",
        "            nn.Linear(self.mlp_hdim[1], 1, bias=False),\n",
        "        )\n",
        "\n",
        "        self.deform_mlp = nn.Sequential(\n",
        "            nn.Linear(self.out_dim, self.mlp_hdim[0], bias=False),\n",
        "            nn.Linear(self.mlp_hdim[0], self.mlp_hdim[1], bias=False),\n",
        "            nn.Linear(self.mlp_hdim[1], 3, bias=False),\n",
        "        )\n",
        "\n",
        "        self.feature_mlp = nn.Sequential(\n",
        "            nn.Linear(self.out_dim, self.mlp_hdim[0], bias=False),\n",
        "            nn.Linear(self.mlp_hdim[0], self.mlp_hdim[1], bias=False),\n",
        "            nn.Linear(self.mlp_hdim[1], self.mlp_hdim[1], bias=False),\n",
        "        )\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, inputs, adj):\n",
        "\n",
        "        x = self.gcn_res[0](inputs, adj)\n",
        "\n",
        "        sdf = self.sdf_mlp(x)\n",
        "        deform = self.deform_mlp(x)\n",
        "        feature = self.feature_mlp(x)\n",
        "        \n",
        "        return sdf, deform, feature\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5GGfOyM3NRu"
      },
      "outputs": [],
      "source": [
        "def gcn_loss(iterations, mesh_verts, mesh_faces, gt_verts, gt_faces, it):\n",
        "\n",
        "    #surface alignment loss\n",
        "    m = pytorch3d.structures.Meshes([mesh_verts, gt_verts], [mesh_faces, gt_faces])\n",
        "  \n",
        "    # pc = pytorch3d.ops.sample_points_from_meshes(m, num_samples=50000)\n",
        "    pred_points = kaolin.ops.mesh.sample_points(mesh_verts.unsqueeze(0), mesh_faces, 50000)[0][0]\n",
        "    gt_points = kaolin.ops.mesh.sample_points(gt_verts.unsqueeze(0), gt_faces, 50000)[0][0]\n",
        "\n",
        "    # pred_points,gt_points = pc[0], pc[1]\n",
        "    chamfer = kaolin.metrics.pointcloud.chamfer_distance(pred_points.unsqueeze(0), gt_points.unsqueeze(0)).mean()\n",
        "    \n",
        "    norm = pytorch3d.loss.mesh_normal_consistency(m)\n",
        "    if it > iterations//2:\n",
        "      lap = pytorch3d.loss.mesh_laplacian_smoothing(m)\n",
        "      return 500*chamfer + lap * laplacian_weight + 1e-6*norm # + sdf_loss + deform_loss\n",
        "    return 500*chamfer + 1e-6*norm #+ sdf_loss + deform_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zzjoso_kziVb"
      },
      "source": [
        "# Train GCN Model for Surface Refinement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QO7CuxCzohb"
      },
      "outputs": [],
      "source": [
        "CONFIG_GCNRES = {\n",
        "    'in_dim': 68,\n",
        "    'hidden_dim': [128, 256],\n",
        "    'out_dim': 128,\n",
        "    'activation': True,\n",
        "    'mlp_hdim': [128,64],\n",
        "    'mlp_odim': 68,\n",
        "}\n",
        "\n",
        "# Same set of Hyperparam is applied\n",
        "lr = 1e-4\n",
        "laplacian_weight = 0.1\n",
        "gcn_iterations = 5000\n",
        "save_every = 100\n",
        "multires = 2\n",
        "grid_res = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJ4L--ys4rh2"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def gcn_pretrain(iterations, gcn_model, optimizer, pred_sdf, tets_verts, tets, tets_verts_features, gt_verts, gt_faces, thresh):\n",
        "  gcn_model.train()\n",
        "\n",
        "  assert thresh >= 0\n",
        "\n",
        "  surf_tets_verts_idx, surf_tets_faces = extract_tet(tets, pred_sdf, thresh)\n",
        "  surf_tets_verts = torch.clone(tets_verts[surf_tets_verts_idx])\n",
        "\n",
        "  surf_tets_verts_features = torch.clone(tets_verts_features[surf_tets_verts_idx])\n",
        "  surf_sdfs = pred_sdf[surf_tets_verts_idx].detach()\n",
        "  surf_tets_edges = torch.clone(get_edges(surf_tets_faces).to(device))\n",
        "  \n",
        "  gt_verts, gt_faces = gt_verts.to(device), gt_faces.to(device)\n",
        "\n",
        "  # f = index_vertices_by_faces(gt_verts.unsqueeze(0), gt_faces)\n",
        "\n",
        "  for it in tqdm(range(iterations)):\n",
        "    verts_f = torch.cat((surf_tets_verts, surf_sdfs, surf_tets_verts_features), dim=1)\n",
        "\n",
        "    sdf, deform, fv = gcn_model(verts_f, surf_tets_edges)\n",
        "\n",
        "    update_verts = surf_tets_verts + torch.tanh(deform) / grid_res\n",
        "    update_sdfs = surf_sdfs + sdf\n",
        "\n",
        "    gt_sdfs = get_gt_sdfs(gt_verts.unsqueeze(0), gt_faces, update_verts.unsqueeze(0))\n",
        "\n",
        "    sdf_loss = F.mse_loss(update_sdfs, gt_sdfs, reduction='mean')\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    sdf_loss.backward(retain_graph=True)\n",
        "    optimizer.step()\n",
        "\n",
        "    if it == iterations-1:\n",
        "      print('Iteration {} - loss: {}'.format(iterations, sdf_loss))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pn7Q89vgch9"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Identify surface Tetrahedral\n",
        "input: f = [v, s, F_vol, f_64]\n",
        "output: f = [delta v, delta s, f_64]\n",
        "\n",
        "update v = v + delta v, s = s + delta s\n",
        "generate new meshes by marching tet from v and s; calculate loss\n",
        "\n",
        "loss = chamfer + sdf_loss + deform_loss\n",
        "\"\"\"\n",
        "from pytorch3d import loss\n",
        "\n",
        "def gcn_train(iterations, gcn_model, optimizer, scheduler, pred_sdf, tets_verts, tets, tets_verts_features, gt_verts, gt_faces, thresh):\n",
        "  gcn_model.train()\n",
        "\n",
        "  assert thresh >= 0\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  Firstly, based on predicted sdf value, extract out all surface tetrahedrons\n",
        "  i.e. 1-3 vertices of tetrahedrons are with different sdf signs\n",
        "  return value: \n",
        "  --> surf_tets_idx: origin index of vertices as in tet grid\n",
        "  --> surf_tets_faces: reindexed faces list start from 0\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  surf_tets_verts_idx, surf_tets_faces, non_surf_idx, non_surf_tets = extract_tet(tets, pred_sdf, thresh, True)\n",
        "  surf_tets_edges = get_edges(surf_tets_faces).to(device)\n",
        "\n",
        "  surf_tets_verts = torch.clone(tets_verts[surf_tets_verts_idx])\n",
        "\n",
        "  # obtain per vertex features: \n",
        "  # sdf, position, feature vector\n",
        "\n",
        "  surf_tets_verts_features = torch.clone(tets_verts_features[surf_tets_verts_idx])\n",
        "  surf_sdfs = pred_sdf[surf_tets_verts_idx].detach()\n",
        "  surf_verts_f = torch.cat((surf_tets_verts, surf_sdfs, surf_tets_verts_features), dim=1)\n",
        "\n",
        "  non_surf_verts = torch.clone(tets_verts[non_surf_idx])\n",
        "\n",
        "  # calculate ground truth sdf value over all vertices in tet mesh\n",
        "\n",
        "  # gt_f = index_vertices_by_faces(gt_verts.unsqueeze(0), gt_faces).to(device)\n",
        "  # gt_sdfs, _, _ = point_to_mesh_distance(tets_verts.unsqueeze(0), gt_f)\n",
        "  # gt_sdfs = gt_sdfs.squeeze(0).unsqueeze(1)\n",
        "\n",
        "  gt_verts, gt_faces = gt_verts.to(device), gt_faces.to(device)\n",
        "\n",
        "  gt_f = index_vertices_by_faces(gt_verts.unsqueeze(0), gt_faces)\n",
        "  \n",
        "  gt_sdfs = get_gt_sdfs(gt_verts.unsqueeze(0), gt_faces, tets_verts.unsqueeze(0), gt_f)\n",
        "\n",
        "  for it in range(iterations):\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    Secondly, predict with a 2-layer GCN followed by 2-layer MLP\n",
        "    output:\n",
        "    --> [delta v, delta s, new f_s] \\in R^68\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    sdf, deform, fv = gcn_model(surf_verts_f, surf_tets_edges)\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    Update surface position, sdf, and f_s\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    #updated sdf\n",
        "\n",
        "    us = sdf.detach()\n",
        "    update_sdfs = torch.clone(pred_sdf)\n",
        "    update_sdfs[surf_tets_verts_idx] += us\n",
        "\n",
        "    #update vertices positions\n",
        "\n",
        "    vd = torch.tanh(deform).detach()\n",
        "    update_tets_verts = torch.clone(tets_verts)\n",
        "    update_tets_verts[surf_tets_verts_idx] += vd / grid_res\n",
        "\n",
        "    #update vertices features\n",
        "\n",
        "    vfvs = fv.detach()\n",
        "    update_tets_f = torch.clone(tets_verts_features)\n",
        "    update_tets_f[surf_tets_verts_idx] += vfvs\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Marching Tetrahedra based on new sdf value and deformed vertices in the tet grid\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    mesh_verts, mesh_faces = kaolin.ops.conversions.marching_tetrahedra(update_tets_verts.unsqueeze(0), tets, update_sdfs.squeeze(1).unsqueeze(0))\n",
        "    mesh_verts, mesh_faces = mesh_verts[0], mesh_faces[0]\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Compute Loss for First surface refinement: \n",
        "    Normal consistency + surface alignment + laplacian smooth + sdf L2-reg + deform L2-reg\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # L2 sdf reg: \n",
        "\n",
        "    # Calculate SDF non_surface tetrahedron in newly generated mesh\n",
        "\n",
        "    p_sdfs = get_gt_sdfs(mesh_verts.unsqueeze(0), mesh_faces, non_surf_verts.unsqueeze(0))\n",
        "\n",
        "    # Get ground truth SDF value of non_surface tetrahedron vertices and truncate at +- 0.3 to focus on surface\n",
        "\n",
        "    ns_g = gt_sdfs[non_surf_idx]\n",
        "    mask = (ns_g >= -0.3) & (ns_g <= 0.3)\n",
        "    p = p_sdfs[mask]\n",
        "    g = ns_g[mask]\n",
        "\n",
        "    # Also calculate surface sdf loss\n",
        "\n",
        "    s_sdfs = get_gt_sdfs(gt_verts.unsqueeze(0), gt_faces, update_tets_verts[surf_tets_verts_idx].unsqueeze(0), gt_f)\n",
        "\n",
        "    sdf_loss = F.mse_loss(sdf, s_sdfs - surf_sdfs, reduction='sum') #+F.mse_loss(p, g, reduction='sum') +\n",
        "\n",
        "    #L2 deform reg\n",
        "\n",
        "    deform_loss = F.mse_loss(deform, torch.zeros(deform.shape).to(device), reduction='sum')\n",
        "\n",
        "    #surface alignment loss\n",
        "\n",
        "    r_loss = gcn_loss(iterations, mesh_verts, mesh_faces, gt_verts, gt_faces, it)\n",
        "\n",
        "    loss = r_loss + 0.4*sdf_loss + deform_loss\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    loss.backward(retain_graph=True)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    if (it) % save_every == 0 or it == (iterations - 1): \n",
        "      print ('Iteration {} - loss: {}, # of mesh vertices: {}, # of mesh faces: {}'.format(it, loss, mesh_verts.shape[0], mesh_faces.shape[0]))\n",
        "      \n",
        "      # save reconstructed mesh\n",
        "      timelapse.add_mesh_batch(\n",
        "          iteration=it+1,\n",
        "          category='extracted_mesh',\n",
        "          vertices_list=[mesh_verts.cpu()],\n",
        "          faces_list=[mesh_faces.cpu()]\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oicuzl0WSHRE"
      },
      "outputs": [],
      "source": [
        "from pytorch3d import loss\n",
        "\n",
        "def gcn_train_wsubdiv(iterations, gcn_model, optimizer, scheduler, pred_sdf, tets_verts, tets, tets_verts_features, gt_verts, gt_faces, thresh):\n",
        "  gcn_model.train()\n",
        "\n",
        "  assert thresh >= 0\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  Firstly, based on predicted sdf value, extract out all surface tetrahedrons\n",
        "  i.e. 1-3 vertices of tetrahedrons are with different sdf signs\n",
        "  return value: \n",
        "  --> surf_tets_idx: origin index of vertices as in tet grid\n",
        "  --> surf_tets_faces: reindexed faces list start from 0\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Get surface and non-surface tetrahedron indices\n",
        "\n",
        "  surf_tets_verts_idx, surf_tets_faces, non_surf_idx, non_surf_tets = extract_tet(tets, pred_sdf, thresh, True)\n",
        "\n",
        "  # Use surface faces to obtain edge list\n",
        "\n",
        "  surf_tets_edges = get_edges(surf_tets_faces).to(device)\n",
        "\n",
        "\n",
        "  # Use surface indices obtain per vertex features: \n",
        "\n",
        "  # Surface vertices positions:\n",
        "\n",
        "  surf_tets_verts = torch.clone(tets_verts[surf_tets_verts_idx])\n",
        "\n",
        "  # Surface per vertex feature vector:\n",
        "\n",
        "  surf_tets_verts_features = torch.clone(tets_verts_features[surf_tets_verts_idx])\n",
        "\n",
        "  # Surface per vertex predicted sdf value:\n",
        "\n",
        "  surf_sdfs = torch.clone(pred_sdf[surf_tets_verts_idx])\n",
        "\n",
        "  # Cat all features together as the input into model\n",
        "\n",
        "  surf_verts_f = torch.cat((surf_tets_verts, surf_sdfs, surf_tets_verts_features), dim=1)\n",
        "\n",
        "  # Obtain non_surface vertices position\n",
        "\n",
        "  non_surf_verts = torch.clone(tets_verts[non_surf_idx])\n",
        "\n",
        "  # Calculate ground truth sdf value over all vertices in tet mesh\n",
        "\n",
        "  gt_verts, gt_faces = gt_verts.to(device), gt_faces.to(device)\n",
        "  gt_f = index_vertices_by_faces(gt_verts.unsqueeze(0), gt_faces)\n",
        "\n",
        "  gt_sdfs = get_gt_sdfs(gt_verts.unsqueeze(0), gt_faces, tets_verts.unsqueeze(0), gt_f)\n",
        "\n",
        "  for it in range(100):\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    Secondly, predict with a 2-layer GCN followed by 2-layer MLP\n",
        "    output:\n",
        "    --> [delta v, delta s, new f_s] \\in R^68\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    sdf, deform, fv = gcn_model(surf_verts_f, surf_tets_edges)\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    Update surface position, sdf, and f_s\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    #updated sdf\n",
        "\n",
        "    us = sdf.detach()\n",
        "    update_sdfs = torch.clone(pred_sdf)\n",
        "    update_sdfs[surf_tets_verts_idx] += us\n",
        "\n",
        "    #update vertices positions\n",
        "\n",
        "    vd = torch.tanh(deform).detach()\n",
        "    update_tets_verts = torch.clone(tets_verts)\n",
        "    update_tets_verts[surf_tets_verts_idx] += vd / grid_res\n",
        "\n",
        "    #update vertices features\n",
        "\n",
        "    vfvs = fv.detach()\n",
        "    update_tets_f = torch.clone(tets_verts_features)\n",
        "    update_tets_f[surf_tets_verts_idx] += vfvs\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Volume subdivision:\n",
        "    Re-identify surface tets based on updated sdf value and subdivide identified surface tets\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract surface tetrahedrons based on updated sdf value\n",
        "\n",
        "    d_surf_tets_verts_idx, d_surf_tets_faces = extract_tet(tets, update_sdfs, thresh)\n",
        "    if d_surf_tets_faces.shape[0] == 0:\n",
        "      d_surf_tets_verts_idx, d_surf_tets_faces = surf_tets_verts_idx, surf_tets_faces\n",
        "\n",
        "    # Use new surface indices to get new surface vertices positions\n",
        "\n",
        "    d_surf_tets_verts = torch.clone(update_tets_verts[d_surf_tets_verts_idx])\n",
        "\n",
        "    # Use new surface indices to get new surface per vertex sdf and features\n",
        "\n",
        "    d_surf_tets_verts_features = torch.clone(update_tets_f[d_surf_tets_verts_idx])\n",
        "    d_surf_sdfs = torch.clone(update_sdfs[d_surf_tets_verts_idx])\n",
        "\n",
        "    # Cat new surface per vertex features together for subdivide\n",
        "\n",
        "    d_surf_verts_f = torch.cat((d_surf_sdfs, d_surf_tets_verts_features), dim=1)\n",
        "\n",
        "    # Perform subdivide, returns subdivided surface tetrahedron \n",
        "\n",
        "    d_tets_verts, d_tets_faces, d_tets_fvs = kaolin.ops.mesh.subdivide_tetmesh(d_surf_tets_verts.unsqueeze(0), d_surf_tets_faces, d_surf_verts_f.unsqueeze(0))\n",
        "    d_tets_verts, d_tets_fvs = d_tets_verts[0], d_tets_fvs[0]\n",
        "    d_tets_sdfs = d_tets_fvs[:, 0]\n",
        "\n",
        "\n",
        "    # Get surface edges from tetrahedron faces\n",
        "\n",
        "    d_tets_edges = get_edges(d_tets_faces).to(device)\n",
        "\n",
        "    # Cat to obtain feature input into GCN\n",
        "\n",
        "    d_tets_fvs = torch.cat((d_tets_verts, d_tets_fvs), dim=1)\n",
        "\n",
        "    d_sdf, d_deform, d_fv = gcn_model(d_tets_fvs, d_tets_edges)\n",
        "\n",
        "    # Updated sdf\n",
        "\n",
        "    d_update_sdfs = d_tets_sdfs.unsqueeze(1).detach() + d_sdf #since we need this to back propagate\n",
        "\n",
        "\n",
        "    # Update vertices positions\n",
        "\n",
        "    d_vd = torch.tanh(d_deform).detach()\n",
        "    d_update_tets_verts = d_tets_verts + d_vd / grid_res\n",
        "\n",
        "    # Update vertices features\n",
        "\n",
        "    d_vfvs = d_fv.detach()\n",
        "    d_update_tets_f = d_tets_fvs[:, 4:] + d_vfvs\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Marching Tetrahedra based on new sdf value and deformed vertices in the tet grid\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    mesh_verts, mesh_faces = kaolin.ops.conversions.marching_tetrahedra(d_update_tets_verts.unsqueeze(0), d_tets_faces, d_update_sdfs.squeeze(1).unsqueeze(0))\n",
        "    mesh_verts, mesh_faces = mesh_verts[0], mesh_faces[0]\n",
        "\n",
        "    print(mesh_verts.shape, mesh_faces.shape)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Compute Loss for First surface refinement: \n",
        "    Normal consistency + surface alignment + laplacian smooth + sdf L2-reg + deform L2-reg\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # L2 sdf reg: \n",
        "\n",
        "    # Calculate SDF non_surface tetrahedron in newly generated mesh\n",
        "\n",
        "    p_sdfs = get_gt_sdfs(mesh_verts.unsqueeze(0), mesh_faces, non_surf_verts.unsqueeze(0))\n",
        "\n",
        "    # Get ground truth SDF value of non_surface tetrahedron vertices and truncate at +- 0.3 to focus on surface\n",
        "\n",
        "    ns_g = gt_sdfs[non_surf_idx]\n",
        "    mask = (ns_g >= -0.3) & (ns_g <= 0.3)\n",
        "    p = p_sdfs[mask]\n",
        "    g = ns_g[mask]\n",
        "\n",
        "    # Also calculate surface sdf loss\n",
        "\n",
        "    # s_sdfs, _, _ = point_to_mesh_distance(update_tets_verts[surf_tets_verts_idx].unsqueeze(0), gt_f)\n",
        "    # s_sign = kaolin.ops.mesh.check_sign(gt_verts.unsqueeze(0), gt_faces, update_tets_verts[surf_tets_verts_idx].unsqueeze(0))\n",
        "    # s_sdfs[s_sign==False] *= -1\n",
        "\n",
        "    # s_sdfs = s_sdfs.squeeze(0).unsqueeze(1)\n",
        "\n",
        "    s_sdfs = get_gt_sdfs(gt_verts.unsqueeze(0), gt_faces, d_update_tets_verts.unsqueeze(0), gt_f)\n",
        "\n",
        "    sdf_loss = F.mse_loss(p, g, reduction='sum') + F.mse_loss(d_update_sdfs, s_sdfs, reduction='sum')\n",
        "\n",
        "    #L2 deform reg\n",
        "\n",
        "    deform_loss = F.mse_loss(d_deform, torch.zeros(d_deform.shape).to(device), reduction='sum')\n",
        "\n",
        "    #surface alignment loss\n",
        "\n",
        "    r_loss = gcn_loss(iterations, mesh_verts, mesh_faces, gt_verts, gt_faces, it)\n",
        "    \n",
        "    loss = r_loss + deform_loss + 0.4*sdf_loss \n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    loss.backward(retain_graph=True)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    # print('Iteration {} - loss: {}'.format(it, loss))\n",
        "\n",
        "    if (it) % save_every == 0 or it == (iterations - 1): \n",
        "      print ('Iteration {} - loss: {}, # of mesh vertices: {}, # of mesh faces: {}'.format(it, loss, mesh_verts.shape[0], mesh_faces.shape[0]))\n",
        "      \n",
        "      # save reconstructed mesh\n",
        "      timelapse.add_mesh_batch(\n",
        "          iteration=it+1,\n",
        "          category='extracted_mesh',\n",
        "          vertices_list=[mesh_verts.cpu()],\n",
        "          faces_list=[mesh_faces.cpu()]\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fm5aud4mz6ZJ"
      },
      "outputs": [],
      "source": [
        "refine_model = GCN_Res(CONFIG_GCNRES).to(device)\n",
        "\n",
        "print(refine_model)\n",
        "\n",
        "pre_vars = [p for _, p in refine_model.named_parameters()]\n",
        "pre_optimizer = torch.optim.Adam(pre_vars, lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iADZ6VnsBTCJ"
      },
      "outputs": [],
      "source": [
        "pred_sdfs, f_vs = get_pred_sdfs(sdf_model, tets_verts)\n",
        "print(extract_tet(tets, d, 0.001)[0].shape)\n",
        "print(sample_faces.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Hlf1luSRwZ3"
      },
      "outputs": [],
      "source": [
        "gcn_pretrain(500, refine_model, pre_optimizer, d, tets_verts, tets, f_vs, sample_verts, sample_faces, 0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7ylyiYXI08J"
      },
      "outputs": [],
      "source": [
        "refine_vars = [p for _, p in refine_model.named_parameters()]\n",
        "refine_optimizer = torch.optim.Adam(pre_vars, lr=lr)\n",
        "refine_scheduler = torch.optim.lr_scheduler.LambdaLR(pre_optimizer, lr_lambda=lambda x: max(0.0, 10**(-x*0.0002))) # LR decay over time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqEaSvssI8it"
      },
      "outputs": [],
      "source": [
        "gcn_train(gcn_iterations, refine_model, refine_optimizer, refine_scheduler, d, tets_verts, tets, f_vs, sample_verts, sample_faces, 0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhShD27xy42k"
      },
      "outputs": [],
      "source": [
        "# test_gcn = GCN_Res(CONFIG_GCNRES).to(device)\n",
        "\n",
        "# t_v = [p for _, p in test_gcn.named_parameters()]\n",
        "# t_op = torch.optim.Adam(t_v, lr=lr)\n",
        "# t_sch = torch.optim.lr_scheduler.LambdaLR(t_op, lr_lambda=lambda x: max(0.0, 10**(-x*0.0002))) # LR decay over time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-RPwOOP0bNH"
      },
      "outputs": [],
      "source": [
        "# gcn_pretrain(700, test_gcn, t_op, d, tets_verts, tets, f_vs, sample_verts, sample_faces, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsUxitzdzEja"
      },
      "outputs": [],
      "source": [
        "# gcn_train_wsubdiv(gcn_iterations, test_gcn, t_op, t_sch, d, tets_verts, tets, f_vs, sample_verts, sample_faces, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tx0o-m2bukdz"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "to-dos: volume subdivision\n",
        "\n",
        "identify T_surf's neighbors: i.e. share a same edge\n",
        "\n",
        "subdivide T_surf to perform another surface refinement\n",
        "unsubdivded tet, i.e. not surface's neighbor, is dropped to save memory and computation\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#neightbor if share an edge\n",
        "#identify edge based on surf_faces and adj matrix\n",
        "\n",
        "\"\"\"\n",
        "convert face_lists = [v1, v2, v3, v4] --> \n",
        "[\n",
        "  [v1, v2, v3, E, o1],\n",
        "  [v2, v3, v4, E, o2],\n",
        "  [v1, v2, v4, E, o3],\n",
        "  [v1, v3, v4, E, o4]\n",
        "]\n",
        "\n",
        "by torch combinations, E = tet index, o_i = face_i index\n",
        "\"\"\"\n",
        "def convert_face_lists(tets):\n",
        "  face_idx = torch.tensor([1,2,3,4])\n",
        "  tet_face_list = []\n",
        "  for idx, tet in enumerate(tets):\n",
        "    tet_idx = torch.full(4, idx)\n",
        "    tet_faces = torch.combinations(tet, r=3)\n",
        "    tet_faces = torch.cat((tet_faces, tet_idx, face_idx), dim=1)\n",
        "    tet_face_list.append(tet_faces)\n",
        "\n",
        "  tet_face_list = torch.stack(tet_face_list, dim=0)\n",
        "\n",
        "  def compare(face_1, face_2):\n",
        "    o1 = face_1[0]-face_2[0]\n",
        "    o2 = face_1[1]-face_2[1]\n",
        "    o3 = face_1[2]-face_2[2]\n",
        "\n",
        "    if o1 <= 0 or (o1 == 0 and o2 < 0) or (o1 == 0 and o2 == 0 and o3 < 0):\n",
        "      return -1\n",
        "    elif o1 == 0 and o2 == 0 and o3 == 0:\n",
        "      return 0\n",
        "    else: \n",
        "      return 1\n",
        "\n",
        "  sorted(tet_face_list, cmp=compare)\n",
        "  return tet_face_list\n",
        "\n",
        "def get_neighbor(surf_tet_verts, surf_tets, tet_verts, tets):\n",
        "  tet_face_list = convert_face_lists(tets)\n",
        "  \n",
        "\n",
        "surf_idx, surf = extract_tet(tets, pred_sdfs, 0.003)\n",
        "\n",
        "print(surf_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Too6Yu0KxrRH"
      },
      "outputs": [],
      "source": [
        "def get_faces(input):\n",
        "  c = torch.combinations(torch.arange(input.size(1)), r=3)\n",
        "  x = input[:,None].expand(-1,len(c),-1).cpu()\n",
        "  idx = c[None].expand(len(x), -1, -1)\n",
        "  x = x.gather(dim=2, index=idx)\n",
        "\n",
        "  return x.view(-1, *x.shape[2:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e-1rPxP3Ru9"
      },
      "outputs": [],
      "source": [
        "# a = get_faces(tets)\n",
        "# b = get_faces(tets[surf_idx])\n",
        "\n",
        "# # OPTIMIZE TF OUT OF THIS\n",
        "# for f in b:\n",
        "#   mask = a == f\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H7RCV62USts"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "p68GrPqkUU07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "613d6b37-b257-44f6-a63f-16b4ca015a24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 31.3 MB/s eta 0:00:01\r\u001b[K     |▉                               | 20 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30 kB 22.3 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |███                             | 71 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 81 kB 16.8 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 92 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 102 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 112 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 122 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 133 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██████                          | 143 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 153 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 163 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 174 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 184 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 194 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 204 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 215 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 225 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 235 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 245 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 256 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 266 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 276 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 286 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 296 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 307 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 317 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 327 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 337 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 348 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 358 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 368 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 378 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 389 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 399 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 409 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 419 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 430 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 440 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 450 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 460 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 471 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 481 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 491 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 501 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 512 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 522 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 532 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 542 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 552 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 563 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 573 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 583 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 593 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 604 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 614 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 624 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 634 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 645 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 655 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 665 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 675 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 686 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 696 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 706 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 716 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 727 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 737 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 747 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 757 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 761 kB 13.7 MB/s \n",
            "\u001b[?25h  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "#Use pyngrok to access localhost:80 on Colab\n",
        "\n",
        "!pip install pyngrok --quiet \n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "# Setting the authtoken (optional)\n",
        "# Get authtoken from https://dashboard.ngrok.com/auth\n",
        "NGROK_AUTH_TOKEN = \"2Hzzzh94FgOXssVkSP5Yffz8uYg_By2RMDZLTPx1aXakhYfH\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "rWJ2SS1EUYjn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05baf791-2aef-482c-c3c9-487695c1595d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracking URL: NgrokTunnel: \"http://7a51-34-91-38-1.ngrok.io\" -> \"http://localhost:80\"\n"
          ]
        }
      ],
      "source": [
        "#generating a public url mapped to localhost 80\n",
        "public_url = ngrok.connect(port=80, proto=\"http\", options={\"bind_tls\": True, \"local\": True})\n",
        "print(\"Tracking URL:\", public_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RZLd8x7UrUa"
      },
      "outputs": [],
      "source": [
        "#Start Kaolin Dash3D on localhost:80 \n",
        "!kaolin-dash3d --logdir=/content/drive/MyDrive/CV_DMTet/Logs --port=80"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6lqIYyxq_sN"
      },
      "source": [
        "# Prev Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooT3zo23rA62"
      },
      "outputs": [],
      "source": [
        "# from tqdm import tqdm\n",
        "\n",
        "# # MLP + Positional Encoding\n",
        "# class Decoder(torch.nn.Module):\n",
        "#     def __init__(self, input_dims = 3, internal_dims = 128, output_dims = 4, hidden = 5, multires = 2):\n",
        "#         super().__init__()\n",
        "#         self.embed_fn = None\n",
        "#         if multires > 0:\n",
        "#             embed_fn, input_ch = get_embedder(multires)\n",
        "#             self.embed_fn = embed_fn\n",
        "#             input_dims = input_ch\n",
        "\n",
        "#         # net = (torch.nn.Linear(input_dims, internal_dims, bias=False), torch.nn.ReLU())\n",
        "#         # for i in range(hidden-1):\n",
        "#         #     net = net + (torch.nn.Linear(internal_dims, internal_dims, bias=False), torch.nn.ReLU())\n",
        "#         # net = net + (torch.nn.Linear(internal_dims, output_dims, bias=False),)\n",
        "#         # self.net = torch.nn.Sequential(*net)\n",
        "\n",
        "#         self.net = torch.nn.Sequential(\n",
        "#             torch.nn.Linear(input_dims, 256, bias=False),\n",
        "#             torch.nn.ReLU(),\n",
        "#             torch.nn.Linear(256, 256, bias=False),\n",
        "#             torch.nn.ReLU(),\n",
        "#             torch.nn.Linear(256, 128, bias=False),\n",
        "#             torch.nn.ReLU(),\n",
        "#             torch.nn.Linear(128, 64, bias=False),\n",
        "#             torch.nn.ReLU(),\n",
        "#             torch.nn.Linear(64, output_dims, bias=False),\n",
        "#         )\n",
        "\n",
        "#     def forward(self, p):\n",
        "#         if self.embed_fn is not None:\n",
        "#             p = self.embed_fn(p)\n",
        "#         print(p.shape)\n",
        "#         out = self.net(p)\n",
        "#         return out\n",
        "\n",
        "#     def pre_train_sphere(self, iter):\n",
        "#         print (\"Initialize SDF to sphere\")\n",
        "#         loss_fn = torch.nn.MSELoss()\n",
        "#         optimizer = torch.optim.Adam(list(self.parameters()), lr=1e-4)\n",
        "\n",
        "#         for i in tqdm(range(iter)):\n",
        "#             p = torch.rand((1024,3), device='cuda') - 0.5\n",
        "#             ref_value  = torch.sqrt((p**2).sum(-1)) - 0.3\n",
        "#             output = self(p)\n",
        "#             loss = loss_fn(output[...,0], ref_value)\n",
        "#             optimizer.zero_grad()\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#         print(\"Pre-trained MLP\", loss.item())\n",
        "\n",
        "\n",
        "# # Positional Encoding from https://github.com/yenchenlin/nerf-pytorch/blob/1f064835d2cca26e4df2d7d130daa39a8cee1795/run_nerf_helpers.py\n",
        "# class Embedder:\n",
        "#     def __init__(self, **kwargs):\n",
        "#         self.kwargs = kwargs\n",
        "#         self.create_embedding_fn()\n",
        "        \n",
        "#     def create_embedding_fn(self):\n",
        "#         embed_fns = []\n",
        "#         d = self.kwargs['input_dims']\n",
        "#         out_dim = 0\n",
        "#         if self.kwargs['include_input']:\n",
        "#             embed_fns.append(lambda x : x)\n",
        "#             out_dim += d\n",
        "            \n",
        "#         max_freq = self.kwargs['max_freq_log2']\n",
        "#         N_freqs = self.kwargs['num_freqs']\n",
        "        \n",
        "#         if self.kwargs['log_sampling']:\n",
        "#             freq_bands = 2.**torch.linspace(0., max_freq, steps=N_freqs)\n",
        "#         else:\n",
        "#             freq_bands = torch.linspace(2.**0., 2.**max_freq, steps=N_freqs)\n",
        "            \n",
        "#         for freq in freq_bands:\n",
        "#             for p_fn in self.kwargs['periodic_fns']:\n",
        "#                 embed_fns.append(lambda x, p_fn=p_fn, freq=freq : p_fn(x * freq))\n",
        "#                 out_dim += d\n",
        "                    \n",
        "#         self.embed_fns = embed_fns\n",
        "#         self.out_dim = out_dim\n",
        "        \n",
        "#     def embed(self, inputs):\n",
        "#         return torch.cat([fn(inputs) for fn in self.embed_fns], -1)\n",
        "\n",
        "# def get_embedder(multires):\n",
        "#     embed_kwargs = {\n",
        "#                 'include_input' : True,\n",
        "#                 'input_dims' : 3,\n",
        "#                 'max_freq_log2' : multires-1,\n",
        "#                 'num_freqs' : multires,\n",
        "#                 'log_sampling' : True,\n",
        "#                 'periodic_fns' : [torch.sin, torch.cos],\n",
        "#     }\n",
        "    \n",
        "#     embedder_obj = Embedder(**embed_kwargs)\n",
        "#     embed = lambda x, eo=embedder_obj : eo.embed(x)\n",
        "#     return embed, embedder_obj.out_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixiLERLsrDz3"
      },
      "outputs": [],
      "source": [
        "# test_model = Decoder(multires=2).to(device)\n",
        "# pred = test_model(tets_verts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24GAvO6IrKbC"
      },
      "outputs": [],
      "source": [
        "# def sdf_train(iterations, model, optimizer):\n",
        "#   model.eval()\n",
        "#   for it in range(iterations):\n",
        "#       pred = model(tet_verts) # predict SDF and per-vertex deformation\n",
        "#       sdf, deform = pred[:,0], pred[:,1:]\n",
        "#       verts_deformed = tet_verts + torch.tanh(deform) / grid_res # constraint deformation to avoid flipping tets\n",
        "#       mesh_verts, mesh_faces = kaolin.ops.conversions.marching_tetrahedra(verts_deformed.unsqueeze(0), tets, sdf.unsqueeze(0)) # running MT (batched) to extract surface mesh\n",
        "#       mesh_verts, mesh_faces = mesh_verts[0], mesh_faces[0]\n",
        "#       loss = loss_f(mesh_verts, mesh_faces, sample_verts, sample_faces, it)\n",
        "#       optimizer.zero_grad()\n",
        "#       loss.backward()\n",
        "#       optimizer.step()\n",
        "#       scheduler.step()\n",
        "#       if (it) % save_every == 0 or it == (iterations - 1): \n",
        "#           print ('Iteration {} - loss: {}, # of mesh vertices: {}, # of mesh faces: {}'.format(it, loss, mesh_verts.shape[0], mesh_faces.shape[0]))\n",
        "#           sign = kaolin.ops.mesh.check_sign(sample_verts.unsqueeze(0).to(device), sample_faces.to(device), verts_deformed.unsqueeze(0))[0]\n",
        "#           print(sign[sign == True])\n",
        "#           # save reconstructed mesh\n",
        "#           timelapse.add_mesh_batch(\n",
        "#               iteration=it+1,\n",
        "#               category='extracted_mesh',\n",
        "#               vertices_list=[mesh_verts.cpu()],\n",
        "#               faces_list=[mesh_faces.cpu()]\n",
        "#           )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9ZyP7cuGPR7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Zzjoso_kziVb"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}