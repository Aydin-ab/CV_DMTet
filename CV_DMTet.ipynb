{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aydin-ab/CV_DMTet/blob/main/CV_DMTet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Dependencies: Kaolin and PyTorch3D"
      ],
      "metadata": {
        "id": "1Af_jBwcMwG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from pathlib import Path\n",
        "INSTALL_PATH = Path(\"/content/drive/'My Drive'/CV_DMTet/\")\n",
        "%cd  $INSTALL_PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soEkYrVqLMhx",
        "outputId": "2f0b15aa-667d-436d-a015-c405017540b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/CV_DMTet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9x7VxtwyMthl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e44a8189-03b2-4c20-bca3-fe71f88ac396"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: Cython 0.29.32\n",
            "Uninstalling Cython-0.29.32:\n",
            "  Successfully uninstalled Cython-0.29.32\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 7.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 24.4 MB 665 kB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# reinstall cython, install usd-core (for 3D rendering), and clone into kaolin repo\n",
        "!pip uninstall Cython --yes\n",
        "!pip install Cython==0.29.20  --quiet\n",
        "!pip install usd-core --quiet\n",
        "# !git clone --recursive https://github.com/NVIDIAGameWorks/kaolin\n",
        "# %cd kaolin"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# installing kaolin and check version\n",
        "%env IGNORE_TORCH_VER=1\n",
        "%env KAOLIN_INSTALL_EXPERIMENTAL=1\n",
        "KAOLIN_PATH = INSTALL_PATH / \"kaolin\"\n",
        "%cd $INSTALL_PATH\n",
        "!if [ ! -d $KAOLIN_PATH ]; then git clone --recursive https://github.com/NVIDIAGameWorks/kaolin; fi;\n",
        "%cd $KAOLIN_PATH\n",
        "SETUP_CHECK = KAOLIN_PATH / \"kaolin\" / \"version.py\"\n",
        "!echo Checking if $SETUP_CHECK exists\n",
        "!if [ ! -f $SETUP_CHECK ]; then python setup.py develop; fi;\n",
        "!python -c \"import kaolin; print(kaolin.__version__)\""
      ],
      "metadata": {
        "id": "TCC2bsPsNXcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9c8386b-fa21-4b33-80b7-681579e418a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: IGNORE_TORCH_VER=1\n",
            "env: KAOLIN_INSTALL_EXPERIMENTAL=1\n",
            "/content/drive/My Drive/CV_DMTet\n",
            "/content/drive/My Drive/CV_DMTet/kaolin\n",
            "Checking if /content/drive/My Drive/CV_DMTet/kaolin/kaolin/version.py exists\n",
            "0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# installing PyTorch3D (for rendering)\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "need_pytorch3d=False\n",
        "try:\n",
        "    import pytorch3d\n",
        "except ModuleNotFoundError:\n",
        "    need_pytorch3d=True\n",
        "if need_pytorch3d:\n",
        "    if torch.__version__.startswith(\"1.12.\") and sys.platform.startswith(\"linux\"):\n",
        "        # We try to install PyTorch3D via a released wheel.\n",
        "        pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
        "        version_str=\"\".join([\n",
        "            f\"py3{sys.version_info.minor}_cu\",\n",
        "            torch.version.cuda.replace(\".\",\"\"),\n",
        "            f\"_pyt{pyt_version_str}\"\n",
        "        ])\n",
        "        !pip install fvcore iopath\n",
        "        !pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
        "    else:\n",
        "        # We try to install PyTorch3D from source.\n",
        "        !curl -LO https://github.com/NVIDIA/cub/archive/1.10.0.tar.gz\n",
        "        !tar xzf 1.10.0.tar.gz\n",
        "        os.environ[\"CUB_HOME\"] = os.getcwd() + \"/cub-1.10.0\"\n",
        "        !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'"
      ],
      "metadata": {
        "id": "8kEdknk9OlYc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49570e04-dd7d-4168-b433-046e7bfa0a8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221122.tar.gz (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting iopath\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore) (1.21.6)\n",
            "Collecting yacs>=0.1.6\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from fvcore) (6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fvcore) (4.64.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from fvcore) (2.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from fvcore) (7.1.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from fvcore) (0.8.10)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.7/dist-packages (from iopath) (4.1.1)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221122-py3-none-any.whl size=61484 sha256=dfa28825dfa80e4e699cfc0bc547f420f6d2f011486d514846ddad4e39f08176\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/e4/d7/be0b4010933f5fffea6385e9b319eac9d6e56c82ee4a0164e5\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31547 sha256=6d4c3ffc11329df169a8120005053b92843566566dd03a507a63bc0bfbcc2687\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/cc/ed/ca4e88beef656b01c84b9185196513ef2faf74a5a379b043a7\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: portalocker, yacs, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221122 iopath-0.1.10 portalocker-2.6.0 yacs-0.1.8\n",
            "Looking in links: https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py37_cu113_pyt1121/download.html\n",
            "Collecting pytorch3d\n",
            "  Downloading https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py37_cu113_pyt1121/pytorch3d-0.7.1-cp37-cp37m-linux_x86_64.whl (47.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.2 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fvcore in /usr/local/lib/python3.7/dist-packages (from pytorch3d) (0.1.5.post20221122)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.7/dist-packages (from pytorch3d) (0.1.10)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorch3d) (6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorch3d) (1.21.6)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorch3d) (0.8.10)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorch3d) (2.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorch3d) (7.1.2)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorch3d) (0.1.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorch3d) (4.64.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from iopath->pytorch3d) (2.6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from iopath->pytorch3d) (4.1.1)\n",
            "Installing collected packages: pytorch3d\n",
            "Successfully installed pytorch3d-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-pFs5-ubDSDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import packages"
      ],
      "metadata": {
        "id": "U-ItNJXfPQPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import kaolin\n",
        "\n",
        "from pytorch3d.datasets import (\n",
        "    R2N2,\n",
        "    ShapeNetCore,\n",
        "    collate_batched_meshes,\n",
        "    render_cubified_voxels,\n",
        ")\n",
        "from pytorch3d.renderer import (\n",
        "    FoVPerspectiveCameras,\n",
        "    PointLights,\n",
        "    RasterizationSettings,\n",
        "    TexturesVertex,\n",
        "    look_at_view_transform,\n",
        ")\n",
        "\n",
        "from kaolin.ops.conversions import (\n",
        "    trianglemeshes_to_voxelgrids,\n",
        "    marching_tetrahedra,\n",
        "    voxelgrids_to_cubic_meshes,\n",
        "    voxelgrids_to_trianglemeshes\n",
        ")\n",
        "\n",
        "from kaolin.io.shapenet import (\n",
        "    ShapeNetV2\n",
        ")\n",
        "\n",
        "from pytorch3d.structures import Meshes\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "!wget https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/utils/plot_image_grid.py\n",
        "from plot_image_grid import image_grid # rendering function\n",
        "\n",
        "# add path for demo utils functions \n",
        "sys.path.append(os.path.abspath(''))"
      ],
      "metadata": {
        "id": "xI6uNv5JPP0i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3e13d06-ec2a-4577-89f0-302bb8473806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-30 12:43:26--  https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/utils/plot_image_grid.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1608 (1.6K) [text/plain]\n",
            "Saving to: ‘plot_image_grid.py.1’\n",
            "\n",
            "plot_image_grid.py. 100%[===================>]   1.57K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2022-11-30 12:43:26 (622 KB/s) - ‘plot_image_grid.py.1’ saved [1608/1608]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Dataset: Subset of ShapeNetV2"
      ],
      "metadata": {
        "id": "pKhngPBlPbTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yMSLuRoDPfkp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "707fc6a6-e239-407f-bf36-69b06a93c418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    torch.cuda.set_device(device)\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "aTCgE2WCPiAr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50f813d2-d440-4e19-faeb-630d88bb5792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "zakMDXAEltL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SHAPENET_PATH = \"/content/drive/MyDrive/CV_DMTet/Core\"\n",
        "# SHAPENET_PATH = \"/content/drive/MyDrive/FALL 2022/Computer Vision/Project/Core\"\n",
        "# SHAPENET_ZIP = Path(SHAPENET_PATH) / \"ShapeNetCore.v2.zip\"\n",
        "# !curl \"https://drive.google.com/drive/folders/1WKoRDtHdqubpHMam4iVOMe8D536KHBOY?usp=share_link\" -o SHAPENET_PATH\n",
        "SYNSETS_IDS = ['02747177', '02801938', '02808440', '02818832', '02828884', '02871439', '02876657', '02880940', '02924116', '02933112']\n",
        "shapenet_train = ShapeNetV2(SHAPENET_PATH, categories=SYNSETS_IDS, output_dict=True)\n",
        "shapenet_test = ShapeNetV2(SHAPENET_PATH, categories=SYNSETS_IDS, output_dict=True, train=False)"
      ],
      "metadata": {
        "id": "HiBJWTztPmL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization of Train model & add 3D checkpoint"
      ],
      "metadata": {
        "id": "_vDh8sagQT74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "shapenet_train contains dict that stores model properties\n",
        "\"\"\"\n",
        "\n",
        "sample_model = shapenet_train[8] # change the index here for different models\n",
        "sample_verts = sample_model['mesh'][0]\n",
        "sample_faces = sample_model['mesh'][1]\n",
        "sample_feats = sample_model['mesh'][4]\n",
        "sample_id = sample_model['name'].split('/')[1]\n",
        "\n",
        "print(sample_model)"
      ],
      "metadata": {
        "id": "IHwZRC7hQTnF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2775b2df-23a9-4254-923e-ad50b8fcf364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'mesh': return_type(vertices=tensor([[ 0.0734, -0.2761, -0.1726],\n",
            "        [ 0.0601,  0.1218, -0.1775],\n",
            "        [ 0.0601, -0.2761, -0.1775],\n",
            "        ...,\n",
            "        [-0.2201, -0.3234, -0.0937],\n",
            "        [-0.2372, -0.3234, -0.0333],\n",
            "        [-0.1880, -0.3234, -0.1476]]), faces=tensor([[   0,    1,    2],\n",
            "        [   1,    0,    3],\n",
            "        [   2,    1,    0],\n",
            "        ...,\n",
            "        [6817, 6814, 6815],\n",
            "        [6814, 6817, 6816],\n",
            "        [6818, 6816, 6817]]), uvs=tensor([[-1.0073,  2.6101],\n",
            "        [-1.0104,  0.1351],\n",
            "        [-1.0073,  0.1351],\n",
            "        ...,\n",
            "        [-2.4010,  2.6101],\n",
            "        [-2.4010,  0.1351],\n",
            "        [-2.3010,  2.6101]]), face_uvs_idx=tensor([[-1, -1, -1],\n",
            "        [-1, -1, -1],\n",
            "        [-1, -1, -1],\n",
            "        ...,\n",
            "        [-1, -1, -1],\n",
            "        [-1, -1, -1],\n",
            "        [-1, -1, -1]]), materials=[{'Kd': tensor([0.3373, 0.3373, 0.3373]), 'Ka': tensor([0., 0., 0.]), 'Ks': tensor([0.4000, 0.4000, 0.4000])}, {'Kd': tensor([0., 0., 0.]), 'Ka': tensor([0., 0., 0.]), 'Ks': tensor([0.4000, 0.4000, 0.4000])}, {'Kd': tensor([0.2275, 0.2275, 0.2275]), 'Ka': tensor([0., 0., 0.]), 'Ks': tensor([0.4000, 0.4000, 0.4000])}, {'Kd': tensor([0.1725, 0.1765, 0.1686]), 'Ka': tensor([0., 0., 0.]), 'Ks': tensor([0.4000, 0.4000, 0.4000])}, {'Kd': tensor([1., 1., 1.]), 'Ka': tensor([0., 0., 0.]), 'Ks': tensor([0.4000, 0.4000, 0.4000])}, {'Kd': tensor([1., 1., 1.]), 'Ka': tensor([0., 0., 0.]), 'Ks': tensor([0.4000, 0.4000, 0.4000])}, {'Kd': tensor([0.8588, 0.1255, 0.1882]), 'Ka': tensor([0., 0., 0.]), 'Ks': tensor([0.4000, 0.4000, 0.4000])}, {'Kd': tensor([0.1137, 0.4510, 0.7294]), 'Ka': tensor([0., 0., 0.]), 'Ks': tensor([0.4000, 0.4000, 0.4000])}, {'Kd': tensor([0.1725, 0.3843, 0.2588]), 'Ka': tensor([0., 0., 0.]), 'Ks': tensor([0.4000, 0.4000, 0.4000])}, {'Kd': tensor([1., 1., 1.]), 'Ka': tensor([0., 0., 0.]), 'Ks': tensor([0.4000, 0.4000, 0.4000])}, {'Kd': tensor([0.1843, 0.2275, 0.1882]), 'Ka': tensor([0., 0., 0.]), 'Ks': tensor([0.4000, 0.4000, 0.4000])}, {'Kd': tensor([0.6078, 0.6196, 0.6118]), 'Ka': tensor([0., 0., 0.]), 'Ks': tensor([0.4000, 0.4000, 0.4000])}, {'Kd': tensor([0.1608, 0.6078, 0.5294]), 'Ka': tensor([0., 0., 0.]), 'Ks': tensor([0.4000, 0.4000, 0.4000])}, {'Kd': tensor([0.7176, 0.7451, 0.7412]), 'Ka': tensor([0., 0., 0.]), 'Ks': tensor([0.4000, 0.4000, 0.4000])}, {'Kd': tensor([0.4314, 0.3608, 0.2980]), 'Ka': tensor([0., 0., 0.]), 'Ks': tensor([0.4000, 0.4000, 0.4000])}, {'Kd': tensor([0.5804, 0.1333, 0.2078]), 'Ka': tensor([0., 0., 0.]), 'Ks': tensor([0.4000, 0.4000, 0.4000])}], materials_order=tensor([[    0,     0],\n",
            "        [    1,    24],\n",
            "        [    0,    24],\n",
            "        [    1,    48],\n",
            "        [    0,    48],\n",
            "        [    1,    72],\n",
            "        [    0,    72],\n",
            "        [    1,    96],\n",
            "        [    0,    96],\n",
            "        [    1,   120],\n",
            "        [    0,   120],\n",
            "        [    1,   144],\n",
            "        [    0,   144],\n",
            "        [    1,   168],\n",
            "        [    0,   168],\n",
            "        [    1,   192],\n",
            "        [    0,   192],\n",
            "        [    1,   216],\n",
            "        [    0,   216],\n",
            "        [    1,   240],\n",
            "        [    0,   240],\n",
            "        [    1,   264],\n",
            "        [    0,   264],\n",
            "        [    1,   288],\n",
            "        [    0,   288],\n",
            "        [    1,   312],\n",
            "        [    0,   312],\n",
            "        [    1,   336],\n",
            "        [    0,   336],\n",
            "        [    1,   360],\n",
            "        [    0,   360],\n",
            "        [    1,   384],\n",
            "        [    0,   384],\n",
            "        [    1,   408],\n",
            "        [    0,   408],\n",
            "        [    1,   432],\n",
            "        [    0,   432],\n",
            "        [    1,   456],\n",
            "        [    0,   456],\n",
            "        [    1,   480],\n",
            "        [    0,   480],\n",
            "        [    1,   504],\n",
            "        [    0,   504],\n",
            "        [    1,   528],\n",
            "        [    0,   528],\n",
            "        [    1,   552],\n",
            "        [    0,   552],\n",
            "        [    1,   576],\n",
            "        [    0,   576],\n",
            "        [    1,   600],\n",
            "        [    0,   600],\n",
            "        [    1,   624],\n",
            "        [    0,   624],\n",
            "        [    1,   648],\n",
            "        [    0,   648],\n",
            "        [    1,   672],\n",
            "        [    0,   672],\n",
            "        [    1,   696],\n",
            "        [    0,   696],\n",
            "        [    1,   720],\n",
            "        [    0,   720],\n",
            "        [    1,   744],\n",
            "        [    0,   744],\n",
            "        [    1,   768],\n",
            "        [    0,   768],\n",
            "        [    1,   792],\n",
            "        [    0,   792],\n",
            "        [    1,   816],\n",
            "        [    0,   816],\n",
            "        [    1,   840],\n",
            "        [    0,   840],\n",
            "        [    1,   864],\n",
            "        [    0,   864],\n",
            "        [    1,  3020],\n",
            "        [    1,  3020],\n",
            "        [    1,  3020],\n",
            "        [    1,  3020],\n",
            "        [    1,  3020],\n",
            "        [    1,  3020],\n",
            "        [    1,  3020],\n",
            "        [    1,  3020],\n",
            "        [    1,  3020],\n",
            "        [    1,  3020],\n",
            "        [    1,  3020],\n",
            "        [    1,  3020],\n",
            "        [    1,  3020],\n",
            "        [    1,  3020],\n",
            "        [    1,  3020],\n",
            "        [    1,  3020],\n",
            "        [    1,  3020],\n",
            "        [    1,  3020],\n",
            "        [    1,  3020],\n",
            "        [    1,  3020],\n",
            "        [    2,  3020],\n",
            "        [    1,  8620],\n",
            "        [    1,  8620],\n",
            "        [    1,  8620],\n",
            "        [    1,  8620],\n",
            "        [    1,  8620],\n",
            "        [    2,  8620],\n",
            "        [    3, 11022],\n",
            "        [    3, 13424],\n",
            "        [    2, 13586],\n",
            "        [    4, 13608],\n",
            "        [    3, 13812],\n",
            "        [    2, 14016],\n",
            "        [    3, 15168],\n",
            "        [    5, 16320],\n",
            "        [    3, 16322],\n",
            "        [    3, 16324],\n",
            "        [    2, 16660],\n",
            "        [    2, 16708],\n",
            "        [    3, 16722],\n",
            "        [    3, 16742],\n",
            "        [    2, 16904],\n",
            "        [    2, 16926],\n",
            "        [    3, 16934],\n",
            "        [    4, 16950],\n",
            "        [    3, 17154],\n",
            "        [    4, 17358],\n",
            "        [    3, 17562],\n",
            "        [    4, 17766],\n",
            "        [    3, 17970],\n",
            "        [    4, 18174],\n",
            "        [    3, 18378],\n",
            "        [    4, 18582],\n",
            "        [    3, 18786],\n",
            "        [    4, 18990],\n",
            "        [    3, 19194],\n",
            "        [    4, 19398],\n",
            "        [    3, 19602],\n",
            "        [    4, 19806],\n",
            "        [    3, 20010],\n",
            "        [    4, 20214],\n",
            "        [    3, 20418],\n",
            "        [    3, 20622],\n",
            "        [    2, 20784],\n",
            "        [    4, 20806],\n",
            "        [    3, 21010],\n",
            "        [    4, 21214],\n",
            "        [    3, 21418],\n",
            "        [    4, 21622],\n",
            "        [    3, 21826],\n",
            "        [    4, 22030],\n",
            "        [    3, 22234],\n",
            "        [    4, 22438],\n",
            "        [    3, 22642],\n",
            "        [    4, 22846],\n",
            "        [    3, 23050],\n",
            "        [    4, 23254],\n",
            "        [    3, 23458],\n",
            "        [    4, 23662],\n",
            "        [    3, 23866],\n",
            "        [    4, 24070],\n",
            "        [    3, 24274],\n",
            "        [    4, 24478],\n",
            "        [    3, 24682],\n",
            "        [    4, 24886],\n",
            "        [    3, 25090],\n",
            "        [    4, 25294],\n",
            "        [    3, 25498],\n",
            "        [    4, 25702],\n",
            "        [    3, 25906],\n",
            "        [    4, 26110],\n",
            "        [    3, 26314],\n",
            "        [    3, 26518],\n",
            "        [    2, 26854],\n",
            "        [    3, 26902],\n",
            "        [    2, 27064],\n",
            "        [    6, 27086],\n",
            "        [    7, 27112],\n",
            "        [    8, 27114],\n",
            "        [    3, 27116],\n",
            "        [    9, 27118],\n",
            "        [   10, 27120],\n",
            "        [   11, 27122],\n",
            "        [   12, 27124],\n",
            "        [   13, 27126],\n",
            "        [   14, 27128],\n",
            "        [   15, 27130],\n",
            "        [    0, 27132],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134],\n",
            "        [    1, 27134]]), vertex_normals=None, face_normals=None), 'name': '02747177/4117be347627a845ba6cf6cbb9f4c2bb', 'path': PosixPath('/content/drive/MyDrive/CV_DMTet/Core/02747177/4117be347627a845ba6cf6cbb9f4c2bb/models/model_normalized.obj'), 'synset': '02747177', 'labels': ['ashcan', 'trash can', 'garbage can', 'wastebin', 'ash bin', 'ash-bin', 'ashbin', 'dustbin', 'trash barrel', 'trash bin']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Voxelization\n",
        "res = 16\n",
        "voxelgrids = trianglemeshes_to_voxelgrids(sample_verts.unsqueeze(0), sample_faces, resolution=res)"
      ],
      "metadata": {
        "id": "NGhV-68EXjdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Can visualize model using Kaolin's 3D checkpoint\n",
        "\n",
        "log_dir = \"/content/drive/MyDrive/CV_DMTet/Logs\"\n",
        "timelapse = kaolin.visualize.Timelapse(log_dir)"
      ],
      "metadata": {
        "id": "A41WC_mBFuYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timelapse.add_mesh_batch(\n",
        "    category='sample',\n",
        "    faces_list=[sample_faces.cpu()],\n",
        "    vertices_list=[sample_verts.cpu()],\n",
        ")"
      ],
      "metadata": {
        "id": "n_8RpY76ZJhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voxel_verts, voxel_faces = voxelgrids_to_cubic_meshes(voxelgrids.to(device))\n",
        "voxel_verts, voxel_faces = voxel_verts[0], voxel_faces[0]"
      ],
      "metadata": {
        "id": "7KeHhnVqJ1te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timelapse.add_mesh_batch(\n",
        "    category='sample_voxel',\n",
        "    faces_list=[voxel_faces.cpu()],\n",
        "    vertices_list=[voxel_verts.cpu()],\n",
        ")"
      ],
      "metadata": {
        "id": "eXXmO_BBHwUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Access Dash3D and checkpoints"
      ],
      "metadata": {
        "id": "dWG2SBF7Hovz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use pyngrok to access localhost:80 on Colab\n",
        "\n",
        "!pip install pyngrok --quiet\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "# Setting the authtoken (optional)\n",
        "# Get authtoken from https://dashboard.ngrok.com/auth\n",
        "NGROK_AUTH_TOKEN = \"2IGctyaa9n7vRBd8qq7pzd0bNKh_2pDFmKRk5Af1QDq295xZ4\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)"
      ],
      "metadata": {
        "id": "kmW3TbmyaCFF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13b8f01c-be60-4cd8-981c-7df13cb38fb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |▉                               | 20 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███                             | 71 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 81 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 92 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 102 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 112 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 122 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 133 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████                          | 143 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 153 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 163 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 174 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 184 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 194 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 204 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 215 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 225 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 235 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 245 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 256 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 266 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 276 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 286 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 296 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 307 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 317 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 327 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 337 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 348 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 358 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 368 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 378 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 389 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 399 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 409 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 419 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 430 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 440 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 450 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 460 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 471 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 481 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 491 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 501 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 512 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 522 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 532 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 542 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 552 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 563 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 573 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 583 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 593 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 604 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 614 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 624 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 634 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 645 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 655 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 665 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 675 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 686 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 696 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 706 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 716 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 727 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 737 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 747 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 757 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 761 kB 7.6 MB/s \n",
            "\u001b[?25h  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generating a public url mapped to localhost 80\n",
        "public_url = ngrok.connect(port=80, proto=\"http\", options={\"bind_tls\": True, \"local\": True})\n",
        "print(\"Tracking URL:\", public_url)"
      ],
      "metadata": {
        "id": "pP-3yfuLuHPK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71b7c3cc-4b2f-4b0f-f59e-4ed238cf30bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracking URL: NgrokTunnel: \"http://50c0-34-125-215-160.ngrok.io\" -> \"http://localhost:80\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Start Kaolin Dash3D on localhost:80\n",
        "!kaolin-dash3d --logdir=/content/drive/MyDrive/CV_DMTet/Logs --port=80"
      ],
      "metadata": {
        "id": "ZC7uQFlVaM37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd99331b-3fe6-4446-ead8-ea0333538a11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: kaolin-dash3d: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "b7r0Jbs3LGPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Data preprocessing: \n",
        "\n",
        "voxelize meshes into resolution of 100 first, then voxelize at lower res (~16) to thicken thin structures\n",
        "After performing marching cubes on low res voxel grid, sample 3000 points from surfaces as input to the generator\n",
        "\n",
        "Initialization: \n",
        "\n",
        "a uniform deformable tetrahedral grid generated by Quartet with resolution ~70 (DMTet)\n",
        "\n",
        "Generator:\n",
        "\n",
        "Receive (v, f(v)), where v is 3D coords, and f(v) is features of v (RGD, uvs, etc.) (Not considering for now)\n",
        "Output a predicted s(v), sdf for v, and a deform vector [v1, v2, v3] to perform marching tetrahedra\n",
        "Loss = Chamfer L2 + Laplacian regularization\n",
        "\n",
        "A feature vector will also be output for surface refinement (Not considering for now)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "BIrKe5tFpKha",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "38e060c0-9d34-4846-9ea7-d3b7e0c1cf68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nData preprocessing: \\n\\nvoxelize meshes into resolution of 100 first, then voxelize at lower res (~16) to thicken thin structures\\nAfter performing marching cubes on low res voxel grid, sample 3000 points from surfaces as input to the generator\\n\\nInitialization: \\n\\na uniform deformable tetrahedral grid generated by Quartet with resolution ~70 (DMTet)\\n\\nGenerator:\\n\\nReceive (v, f(v)), where v is 3D coords, and f(v) is features of v (RGD, uvs, etc.) (Not considering for now)\\nOutput a predicted s(v), sdf for v, and a deform vector [v1, v2, v3] to perform marching tetrahedra\\nLoss = Chamfer L2 + Laplacian regularization\\n\\nA feature vector will also be output for surface refinement (Not considering for now)\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparams\n",
        "res=32 # resolution of voxelgrid\n",
        "chamfer_sample_num=20000 # num of sampled point when evaluate chamfer distance\n",
        "lr = 1e-3\n",
        "laplacian_weight = 0.1\n",
        "iterations = 5000\n",
        "save_every = 100\n",
        "multires = 2\n",
        "grid_res = 128"
      ],
      "metadata": {
        "id": "HnV8A6_oJLXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kaolin.ops.mesh import (\n",
        "    sample_points\n",
        ")\n",
        "\n",
        "from kaolin.metrics.pointcloud import (\n",
        "    chamfer_distance\n",
        ")\n",
        "\n",
        "#randomly sample points from predicted mesh and ground truth mesh and calculate Chamfer distance\n",
        "def get_Chamfer_L2(pred_verts, pred_faces, gt_verts, gt_faces):\n",
        "\n",
        "    # generating point cloud using verts and faces\n",
        "\n",
        "    pred_sample = sample_points(\n",
        "        vertices=pred_verts.unsqueeze(0),\n",
        "        faces=pred_faces,\n",
        "        num_samples=chamfer_sample_num\n",
        "    )\n",
        "\n",
        "    gt_sample = sample_points(\n",
        "        vertices=gt_verts.unsqueeze(0),\n",
        "        faces=gt_faces,\n",
        "        num_samples=chamfer_sample_num\n",
        "    )\n",
        "\n",
        "    pred_point_cloud = pred_sample[0].to(device)\n",
        "    gt_point_cloud = gt_sample[0].to(device)\n",
        "\n",
        "    # calculating chamfer distance\n",
        "    dist = chamfer_distance(pred_point_cloud, gt_point_cloud)\n",
        "\n",
        "    return dist"
      ],
      "metadata": {
        "id": "RCy1lvMaKf4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = get_Chamfer_L2(voxel_verts, voxel_faces, sample_verts, sample_faces)\n",
        "print('Chamfer L2 distance between voxelization and ground truth: ', d)"
      ],
      "metadata": {
        "id": "ZkdAO6EfMRu3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "799d590b-80c2-41a6-8e00-ce701361c75a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chamfer L2 distance between voxelization and ground truth:  tensor([229.1679], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Laplacian regularization using umbrella operator (Fujiwara / Desbrun).\n",
        "# https://mgarland.org/class/geom04/material/smoothing.pdf\n",
        "def laplace_regularizer_const(mesh_verts, mesh_faces):\n",
        "    term = torch.zeros_like(mesh_verts)\n",
        "    norm = torch.zeros_like(mesh_verts[..., 0:1])\n",
        "\n",
        "    v0 = mesh_verts[mesh_faces[:, 0], :]\n",
        "    v1 = mesh_verts[mesh_faces[:, 1], :]\n",
        "    v2 = mesh_verts[mesh_faces[:, 2], :]\n",
        "\n",
        "    term.scatter_add_(0, mesh_faces[:, 0:1].repeat(1,3), (v1 - v0) + (v2 - v0))\n",
        "    term.scatter_add_(0, mesh_faces[:, 1:2].repeat(1,3), (v0 - v1) + (v2 - v1))\n",
        "    term.scatter_add_(0, mesh_faces[:, 2:3].repeat(1,3), (v0 - v2) + (v1 - v2))\n",
        "\n",
        "    two = torch.ones_like(v0) * 2.0\n",
        "    norm.scatter_add_(0, mesh_faces[:, 0:1], two)\n",
        "    norm.scatter_add_(0, mesh_faces[:, 1:2], two)\n",
        "    norm.scatter_add_(0, mesh_faces[:, 2:3], two)\n",
        "\n",
        "    term = term / torch.clamp(norm, min=1.0)\n",
        "\n",
        "    return torch.mean(term**2)\n",
        "\n",
        "def loss_f(mesh_verts, mesh_faces, points, it):\n",
        "    pred_points = kaolin.ops.mesh.sample_points(mesh_verts.unsqueeze(0), mesh_faces, chamfer_sample_num)[0][0]\n",
        "    chamfer = kaolin.metrics.pointcloud.chamfer_distance(pred_points.unsqueeze(0), points.unsqueeze(0)).mean()\n",
        "    if it > iterations//2:\n",
        "        lap = laplace_regularizer_const(mesh_verts, mesh_faces)\n",
        "        return chamfer + lap * laplacian_weight\n",
        "    return chamfer"
      ],
      "metadata": {
        "id": "w8waaHk0I0Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "# MLP + Positional Encoding\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, input_dims = 3, internal_dims = 128, output_dims = 4, hidden = 5, multires = 2):\n",
        "        super().__init__()\n",
        "        self.embed_fn = None\n",
        "        if multires > 0:\n",
        "            embed_fn, input_ch = get_embedder(multires)\n",
        "            self.embed_fn = embed_fn\n",
        "            input_dims = input_ch\n",
        "\n",
        "        # net = (torch.nn.Linear(input_dims, internal_dims, bias=False), torch.nn.ReLU())\n",
        "        # for i in range(hidden-1):\n",
        "        #     net = net + (torch.nn.Linear(internal_dims, internal_dims, bias=False), torch.nn.ReLU())\n",
        "        # net = net + (torch.nn.Linear(internal_dims, output_dims, bias=False),)\n",
        "        # self.net = torch.nn.Sequential(*net)\n",
        "\n",
        "        self.net = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_dims, 256, bias=False),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(256, 256, bias=False),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(256, 128, bias=False),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, 64, bias=False),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(64, output_dims, bias=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, p):\n",
        "        if self.embed_fn is not None:\n",
        "            p = self.embed_fn(p)\n",
        "        out = self.net(p)\n",
        "        return out\n",
        "\n",
        "    def pre_train_sphere(self, iter):\n",
        "        print (\"Initialize SDF to sphere\")\n",
        "        loss_fn = torch.nn.MSELoss()\n",
        "        optimizer = torch.optim.Adam(list(self.parameters()), lr=1e-4)\n",
        "\n",
        "        for i in tqdm(range(iter)):\n",
        "            p = torch.rand((1024,3), device='cuda') - 0.5\n",
        "            ref_value  = torch.sqrt((p**2).sum(-1)) - 0.3\n",
        "            output = self(p)\n",
        "            loss = loss_fn(output[...,0], ref_value)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Pre-trained MLP\", loss.item())\n",
        "\n",
        "\n",
        "# Positional Encoding from https://github.com/yenchenlin/nerf-pytorch/blob/1f064835d2cca26e4df2d7d130daa39a8cee1795/run_nerf_helpers.py\n",
        "class Embedder:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.kwargs = kwargs\n",
        "        self.create_embedding_fn()\n",
        "        \n",
        "    def create_embedding_fn(self):\n",
        "        embed_fns = []\n",
        "        d = self.kwargs['input_dims']\n",
        "        out_dim = 0\n",
        "        if self.kwargs['include_input']:\n",
        "            embed_fns.append(lambda x : x)\n",
        "            out_dim += d\n",
        "            \n",
        "        max_freq = self.kwargs['max_freq_log2']\n",
        "        N_freqs = self.kwargs['num_freqs']\n",
        "        \n",
        "        if self.kwargs['log_sampling']:\n",
        "            freq_bands = 2.**torch.linspace(0., max_freq, steps=N_freqs)\n",
        "        else:\n",
        "            freq_bands = torch.linspace(2.**0., 2.**max_freq, steps=N_freqs)\n",
        "            \n",
        "        for freq in freq_bands:\n",
        "            for p_fn in self.kwargs['periodic_fns']:\n",
        "                embed_fns.append(lambda x, p_fn=p_fn, freq=freq : p_fn(x * freq))\n",
        "                out_dim += d\n",
        "                    \n",
        "        self.embed_fns = embed_fns\n",
        "        self.out_dim = out_dim\n",
        "        \n",
        "    def embed(self, inputs):\n",
        "        return torch.cat([fn(inputs) for fn in self.embed_fns], -1)\n",
        "\n",
        "def get_embedder(multires):\n",
        "    embed_kwargs = {\n",
        "                'include_input' : True,\n",
        "                'input_dims' : 3,\n",
        "                'max_freq_log2' : multires-1,\n",
        "                'num_freqs' : multires,\n",
        "                'log_sampling' : True,\n",
        "                'periodic_fns' : [torch.sin, torch.cos],\n",
        "    }\n",
        "    \n",
        "    embedder_obj = Embedder(**embed_kwargs)\n",
        "    embed = lambda x, eo=embedder_obj : eo.embed(x)\n",
        "    return embed, embedder_obj.out_dim"
      ],
      "metadata": {
        "id": "a3FlCXRAHuu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SDF_train(epoch, model, trainset, optimizer):\n",
        "    \"\"\"\n",
        "    for point cloud, extract 3D feature vectors by PVCNN and cat them into an R^1 vector F(v,x)\n",
        "    one copy will be used here for prediction of SDF\n",
        "\n",
        "    for voxel, extract 3D feature vector by sampling on surface\n",
        "\n",
        "    initial prediction will consist of four layers with dimensions:\n",
        "    256, 256, 128, 64\n",
        "\n",
        "    losses will be Chamfer L2 distance between pred and gt\n",
        "\n",
        "    check in kaolin's documentation for how to batch data\n",
        "    \"\"\"\n",
        "\n",
        "    #dataset is a list of model ditionaries\n",
        "    for idx, model_dict in enumerate(trainset):\n",
        "\n",
        "      #get vertices and faces of meshes\n",
        "\n",
        "      mesh = model_dict['mesh'] \n",
        "      verts = mesh[0]\n",
        "      faces = mesh[1]\n",
        "\n",
        "      #voxelize 3D meshes\n",
        "\n",
        "      voxel_grid = trianglemeshes_to_voxelgrids(verts.unsqueeze(0), faces, resolution=res)\n",
        "\n",
        "      "
      ],
      "metadata": {
        "id": "QGYpBz-iwaoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pOF_XimhDPKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "byUtAiNG18Ty"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}